{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"causality A small repo in which I play with the ideas of causal modelling. why this repo exists I'm interested in causal modelling; having read Judea Pearl's The Book of Why: The New Science of Cause and Effect , I then followed up with Jonas Peters' mini-course on causality . Pearl's book is a good layman's introduction to the history of causal inference research, even if mostly written from the viewpoint of one deeply invested in the field. Peters' lecture series turns out to be a great follow-up to the book. getting started installation To get started, install the packages as specified in the environment.yml conda specification file. $ conda env create -f environment.yml If you prefer to install by pip, install the packages listed there manually. (They are all available on PyPI.) running the notebooks There are two options for running the notebooks. The first one is the simplest: just click on the Binder icon below. The second way assumes you've cloned the repository locally, or have downloaded and unzipped it from GitHub. In your terminal (or command prompt), run the following commands: $ source activate causality $ jupyter lab contributing If you are an expert on causal modelling, and see issues with my notebooks, I would love to hear about them! Please post it as an issue . I would also love to accept a pull request.","title":"Welcome"},{"location":"#causality","text":"A small repo in which I play with the ideas of causal modelling.","title":"causality"},{"location":"#why-this-repo-exists","text":"I'm interested in causal modelling; having read Judea Pearl's The Book of Why: The New Science of Cause and Effect , I then followed up with Jonas Peters' mini-course on causality . Pearl's book is a good layman's introduction to the history of causal inference research, even if mostly written from the viewpoint of one deeply invested in the field. Peters' lecture series turns out to be a great follow-up to the book.","title":"why this repo exists"},{"location":"#getting-started","text":"","title":"getting started"},{"location":"#installation","text":"To get started, install the packages as specified in the environment.yml conda specification file. $ conda env create -f environment.yml If you prefer to install by pip, install the packages listed there manually. (They are all available on PyPI.)","title":"installation"},{"location":"#running-the-notebooks","text":"There are two options for running the notebooks. The first one is the simplest: just click on the Binder icon below. The second way assumes you've cloned the repository locally, or have downloaded and unzipped it from GitHub. In your terminal (or command prompt), run the following commands: $ source activate causality $ jupyter lab","title":"running the notebooks"},{"location":"#contributing","text":"If you are an expert on causal modelling, and see issues with my notebooks, I would love to hear about them! Please post it as an issue . I would also love to accept a pull request.","title":"contributing"},{"location":"01-causality-linear-simulation/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import networkx as nx import matplotlib.pyplot as plt import numpy as np import numpy.random as npr import pandas as pd from causality_notes import draw_graph , noise Introduction In this notebook, I would like to simulate a complex causal process (with only linear relationships between the variables), but in a complex DAG that isn't just a triangle. Then, I would like to use this simulated data to convince myself that with the right conditioning on variables, we can recover the correct parameters back. Graphical Structure First off, let's assume that there is the following graphical structure. G = nx . DiGraph () G . add_edge ( 'c' , 'x' , coeff = 5 ) G . add_edge ( 'a' , 'x' , coeff = 2 ) G . add_edge ( 'a' , 'k' , coeff =- 3 ) G . add_edge ( 'x' , 'f' , coeff = 6 ) G . add_edge ( 'x' , 'd' , coeff =- 2 ) G . add_edge ( 'd' , 'g' , coeff =- 8 ) G . add_edge ( 'k' , 'y' , coeff = 3 ) G . add_edge ( 'd' , 'y' , coeff = 5 ) G . add_edge ( 'y' , 'h' , coeff =- 4 ) draw_graph ( G ) Written as a set of equations, it might look like the following: n = 1000 # number of samples taken c = noise ( n ) a = noise ( n ) x = 5 * c + 2 * a + noise ( n ) k = - 3 * a + noise ( n ) f = 6 * x + noise ( n ) d = - 2 * x + noise ( n ) g = - 8 * d + noise ( n ) y = 3 * k + 5 * d + noise ( n ) h = - 4 * y + noise ( n ) data = dict ( c = c , a = a , x = x , k = k , f = f , d = d , g = g , y = y , h = h ) df = pd . DataFrame ( data ) df . sample ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } c a x k f d g y h 798 -1.078732 0.794306 -3.212282 -0.610186 -19.582504 5.964205 -47.652161 29.263166 -116.295944 136 0.636438 -0.655031 3.899436 2.264376 24.951148 -8.262962 65.838390 -35.451501 142.794698 633 -0.363132 -0.304403 0.237570 1.299515 1.166533 -2.155883 16.125365 -6.459946 25.891101 311 -0.773808 0.740542 -2.616127 -3.296688 -12.936983 5.433091 -43.553788 16.290036 -63.290850 56 1.329955 -0.872511 5.286332 4.875391 31.694392 -9.472437 76.314045 -32.155163 129.815254 Note how the coefficients on the edges are basically the linear multipliers. Before we go on, let's get a feel for how the data are distributed. import seaborn as sns sns . pairplot ( df ) <seaborn.axisgrid.PairGrid at 0x7f32be41dca0> Looking at the pair plot above, we should compute the pairwise correlation between each pair of variables. plt . imshow ( df . corr ( method = 'pearson' ) . values , cmap = 'RdBu' ) plt . xticks ( range ( len ( df . columns )), df . columns ) plt . yticks ( range ( len ( df . columns )), df . columns ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f32abe632b0> Compare the graphical structure below against the correlation plots above. Some things are quite neat. c c and k k are uncorrelated, because there is no causal path from c c to k k . Same goes for c c and a a . On the other hand, c c is causally related to all of the other variables. It has a negative correlation with d d and y y , and this comes directly from Sewall Wright's path rules: positive coeff \\times \\times negative coeff gives a negative coeff. draw_graph ( G ) From the graph, we know that the direct effect of x x on y y is going to be -2 \\times 5 = -10 -2 \\times 5 = -10 (Sewall Wright's path analysis). However, if we only regress y y on x x , the coefficients are going to be wrong, because we have a confounder between x x and y y , primarily originating from a a . Now, let's try na\u00efvely regressing y y on x x , given the causal structure above. from statsmodels.regression.linear_model import OLS model = OLS . from_formula ( 'y ~ x' , data = df ) results = model . fit () results . params Intercept 0.450144 x -10.458158 dtype: float64 We almost recover the correct coefficients, but because we didn't condition on the confounding path from x x to y y , that is, the path x <- a -> k -> y . Thus, we're still off. What if we conditioned on a a ? To condition on a a means adding it as a term in the linear regression. model = OLS . from_formula ( 'y ~ x + a' , data = df ) results = model . fit () results . params Intercept 0.115918 x -9.994394 a -8.780201 dtype: float64 Much better! What if we conditioned on k k only? model = OLS . from_formula ( 'y ~ x + k' , data = df ) results = model . fit () results . params Intercept 0.119636 x -9.978843 k 2.906956 dtype: float64 Wonderful! We get coefficients that are much closer to -10, which is exactly what we had expected. Notice how we also recovered the effect of a a and k k respectively on y y . One thing that is quite nice about this scheme is that if we know the causality structure ahead of time, then we need not condition on every last variable. We needn't even condition on every single variable on the confounding path; conditioning on a single variable in each confounding path is sufficient. This property comes in handy in scenarios where we don't have perfect information: if we weren't able to measure a a , or just forgot to measure it, k k is a sufficiently good variable to condition on. What would happen if we conditioned on a variable that wasn't involved in the causal path from x x to y y ? Let's try conditioning on g g . model = OLS . from_formula ( 'y ~ x + g' , data = df ) results = model . fit () results . params Intercept 0.369591 x -1.572082 g -0.556936 dtype: float64 We are way off! This is because g g is not a confounder of x x and y y , therefore, conditioning on it is the wrong thing to do.","title":"01: Linear Simulation"},{"location":"01-causality-linear-simulation/#introduction","text":"In this notebook, I would like to simulate a complex causal process (with only linear relationships between the variables), but in a complex DAG that isn't just a triangle. Then, I would like to use this simulated data to convince myself that with the right conditioning on variables, we can recover the correct parameters back.","title":"Introduction"},{"location":"01-causality-linear-simulation/#graphical-structure","text":"First off, let's assume that there is the following graphical structure. G = nx . DiGraph () G . add_edge ( 'c' , 'x' , coeff = 5 ) G . add_edge ( 'a' , 'x' , coeff = 2 ) G . add_edge ( 'a' , 'k' , coeff =- 3 ) G . add_edge ( 'x' , 'f' , coeff = 6 ) G . add_edge ( 'x' , 'd' , coeff =- 2 ) G . add_edge ( 'd' , 'g' , coeff =- 8 ) G . add_edge ( 'k' , 'y' , coeff = 3 ) G . add_edge ( 'd' , 'y' , coeff = 5 ) G . add_edge ( 'y' , 'h' , coeff =- 4 ) draw_graph ( G ) Written as a set of equations, it might look like the following: n = 1000 # number of samples taken c = noise ( n ) a = noise ( n ) x = 5 * c + 2 * a + noise ( n ) k = - 3 * a + noise ( n ) f = 6 * x + noise ( n ) d = - 2 * x + noise ( n ) g = - 8 * d + noise ( n ) y = 3 * k + 5 * d + noise ( n ) h = - 4 * y + noise ( n ) data = dict ( c = c , a = a , x = x , k = k , f = f , d = d , g = g , y = y , h = h ) df = pd . DataFrame ( data ) df . sample ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } c a x k f d g y h 798 -1.078732 0.794306 -3.212282 -0.610186 -19.582504 5.964205 -47.652161 29.263166 -116.295944 136 0.636438 -0.655031 3.899436 2.264376 24.951148 -8.262962 65.838390 -35.451501 142.794698 633 -0.363132 -0.304403 0.237570 1.299515 1.166533 -2.155883 16.125365 -6.459946 25.891101 311 -0.773808 0.740542 -2.616127 -3.296688 -12.936983 5.433091 -43.553788 16.290036 -63.290850 56 1.329955 -0.872511 5.286332 4.875391 31.694392 -9.472437 76.314045 -32.155163 129.815254 Note how the coefficients on the edges are basically the linear multipliers. Before we go on, let's get a feel for how the data are distributed. import seaborn as sns sns . pairplot ( df ) <seaborn.axisgrid.PairGrid at 0x7f32be41dca0> Looking at the pair plot above, we should compute the pairwise correlation between each pair of variables. plt . imshow ( df . corr ( method = 'pearson' ) . values , cmap = 'RdBu' ) plt . xticks ( range ( len ( df . columns )), df . columns ) plt . yticks ( range ( len ( df . columns )), df . columns ) plt . colorbar () <matplotlib.colorbar.Colorbar at 0x7f32abe632b0> Compare the graphical structure below against the correlation plots above. Some things are quite neat. c c and k k are uncorrelated, because there is no causal path from c c to k k . Same goes for c c and a a . On the other hand, c c is causally related to all of the other variables. It has a negative correlation with d d and y y , and this comes directly from Sewall Wright's path rules: positive coeff \\times \\times negative coeff gives a negative coeff. draw_graph ( G ) From the graph, we know that the direct effect of x x on y y is going to be -2 \\times 5 = -10 -2 \\times 5 = -10 (Sewall Wright's path analysis). However, if we only regress y y on x x , the coefficients are going to be wrong, because we have a confounder between x x and y y , primarily originating from a a . Now, let's try na\u00efvely regressing y y on x x , given the causal structure above. from statsmodels.regression.linear_model import OLS model = OLS . from_formula ( 'y ~ x' , data = df ) results = model . fit () results . params Intercept 0.450144 x -10.458158 dtype: float64 We almost recover the correct coefficients, but because we didn't condition on the confounding path from x x to y y , that is, the path x <- a -> k -> y . Thus, we're still off. What if we conditioned on a a ? To condition on a a means adding it as a term in the linear regression. model = OLS . from_formula ( 'y ~ x + a' , data = df ) results = model . fit () results . params Intercept 0.115918 x -9.994394 a -8.780201 dtype: float64 Much better! What if we conditioned on k k only? model = OLS . from_formula ( 'y ~ x + k' , data = df ) results = model . fit () results . params Intercept 0.119636 x -9.978843 k 2.906956 dtype: float64 Wonderful! We get coefficients that are much closer to -10, which is exactly what we had expected. Notice how we also recovered the effect of a a and k k respectively on y y . One thing that is quite nice about this scheme is that if we know the causality structure ahead of time, then we need not condition on every last variable. We needn't even condition on every single variable on the confounding path; conditioning on a single variable in each confounding path is sufficient. This property comes in handy in scenarios where we don't have perfect information: if we weren't able to measure a a , or just forgot to measure it, k k is a sufficiently good variable to condition on. What would happen if we conditioned on a variable that wasn't involved in the causal path from x x to y y ? Let's try conditioning on g g . model = OLS . from_formula ( 'y ~ x + g' , data = df ) results = model . fit () results . params Intercept 0.369591 x -1.572082 g -0.556936 dtype: float64 We are way off! This is because g g is not a confounder of x x and y y , therefore, conditioning on it is the wrong thing to do.","title":"Graphical Structure"},{"location":"02-instrument-variables/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import numpy as np import networkx as nx import matplotlib.pyplot as plt import pandas as pd from causality_notes import noise , draw_graph from statsmodels.regression.linear_model import OLS % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Introduction This notebook serves to introduce the use of instrument variables, which can be used in linear models to figure out the effect of x on y in the absence of the ability to measure the known confounder. Note: The method here was introduced to me via Jonas Peters' videos. I believe the assumption of linearity is a strong one: we must assume linear models, otherwise the math doesn't check out. Model Assume we have the following model, specified via linear equations in NumPy code: # Set up model with hidden coefficients (pretend that we know them for # now) and instrument variable. size = 1000 gamma = 2 alpha = 3 beta = 1 delta = - 1 h = 2 * noise ( size ) # the hidden, unmeasured variable. i = 4 * noise ( size ) # the instrument variable x = delta * i + beta * h + 2 * noise ( size ) y = gamma * h + alpha * x + 3 * noise ( size ) Graphically, this looks like the following: G = nx . DiGraph () G . add_edge ( 'x' , 'y' , coeff = alpha ) G . add_edge ( 'h' , 'y' , coeff = gamma ) G . add_edge ( 'h' , 'x' , coeff = beta ) G . add_edge ( 'i' , 'x' , coeff = delta ) draw_graph ( G ) If we were to regress y y directly on x x , we would run into issues: Because we didn't measure the confounder variable h h (believe me for a moment that this is assumed to be true), our coefficients will be way off. To show this, first, let's create the pandas DataFrame that will be used with statsmodels . data = dict ( x = x , i = i , h = h , y = y ) df = pd . DataFrame ( data ) Now, we regress y y on x x , and let's observe the output to see how good of a fit it is. model = OLS . from_formula ( 'y ~ x' , data = df ) results = model . fit () results . params Intercept 0.084582 x 3.338945 dtype: float64 We're close, but not really there. (Remember, though, we wouldn't know this in a real-world scenario, where we might have postulated the presence of a hidden variable but didn't have the know-how to go out and measure it.) In the real-world scenario above, we might want to see if there's an instrument variable to help us out with this problem. The use of an instrumental variable works as such: We first regress the instrument variable x x on i i , to obtain 'estimated' values of \\delta i \\delta i . We then regress y y on \\delta i \\delta i , which gives us the coefficient alpha alpha . Don't believe me? Look at the math below: As a structured causal equation, the graphical model can be expressed as such: y = \\alpha x + \\gamma h + n_{y}$$ $$x = \\delta i + \\beta h + n_{x} y = \\alpha x + \\gamma h + n_{y}$$ $$x = \\delta i + \\beta h + n_{x} where n_{x} n_{x} is the noise that generates uncertainty around x x , and n_{y} n_{y} is the noise that generates uncertainty around y y . Substuting x x into the y y (don't take my word for it, try it yourself!), we get: y = (\\alpha \\beta + \\gamma) h + (\\alpha) (\\delta i) + \\alpha n_{x} + n_{y} y = (\\alpha \\beta + \\gamma) h + (\\alpha) (\\delta i) + \\alpha n_{x} + n_{y} The parentheses have been rearranged intentionally for the variable i i . If we regress x x on i i , we will get an estimate for the value of \\delta \\delta . By then multiplying \\delta \\delta by i i , we will get \"fitted\" values of i i . We can then regress y y on \\delta i \\delta i to get the value of \\alpha \\alpha , which is exactly what we want! Enough in words, let's look at the code for this! Mechanics First, we regress x x on i i . model = OLS . from_formula ( 'x ~ i' , data = df ) results = model . fit () results . params [ 'i' ] -0.9589112040092465 Notice how we get an estimator that is kind of off. It isn't quite accurate, but my gut feeling tells me that's ok. To create the fitted i i , we multiply the learned regression parameter by the original values. df [ '\u03b4i' ] = df [ 'i' ] * results . params [ 'i' ] Then, we regress y y on \\delta i \\delta i : model = OLS . from_formula ( 'y ~ \u03b4i' , data = df ) results = model . fit () results . params Intercept -0.055847 \u03b4i 2.985291 dtype: float64 Voila! We get back the effect of x x on y y by use of this instrument variable i i ! Really happy having seen that it works, and having seen some of the math that goes on behind it! Assumptions Now, all of this sounds good and nice, but it does seem a bit \"magical\", to say the least. After all, \"linearity\" does seem like a very strong assumption. Moreover, in order to use an instrument variable, we have to justify that it has: a causal effect on x x , no causal effect on y y , and no causal effect on h h Indeed, there is no free lunch here: we have to use background knowledge (or other means) to justify why i i is a suitable instrument variable; simply asserting this point is not sufficient. Acknowledgments I would like to thank Maxmillian Gobel for pointing out linguistic inconsistencies in this notebook and raising them with me.","title":"02: Instrument Variables"},{"location":"02-instrument-variables/#introduction","text":"This notebook serves to introduce the use of instrument variables, which can be used in linear models to figure out the effect of x on y in the absence of the ability to measure the known confounder. Note: The method here was introduced to me via Jonas Peters' videos. I believe the assumption of linearity is a strong one: we must assume linear models, otherwise the math doesn't check out.","title":"Introduction"},{"location":"02-instrument-variables/#model","text":"Assume we have the following model, specified via linear equations in NumPy code: # Set up model with hidden coefficients (pretend that we know them for # now) and instrument variable. size = 1000 gamma = 2 alpha = 3 beta = 1 delta = - 1 h = 2 * noise ( size ) # the hidden, unmeasured variable. i = 4 * noise ( size ) # the instrument variable x = delta * i + beta * h + 2 * noise ( size ) y = gamma * h + alpha * x + 3 * noise ( size ) Graphically, this looks like the following: G = nx . DiGraph () G . add_edge ( 'x' , 'y' , coeff = alpha ) G . add_edge ( 'h' , 'y' , coeff = gamma ) G . add_edge ( 'h' , 'x' , coeff = beta ) G . add_edge ( 'i' , 'x' , coeff = delta ) draw_graph ( G ) If we were to regress y y directly on x x , we would run into issues: Because we didn't measure the confounder variable h h (believe me for a moment that this is assumed to be true), our coefficients will be way off. To show this, first, let's create the pandas DataFrame that will be used with statsmodels . data = dict ( x = x , i = i , h = h , y = y ) df = pd . DataFrame ( data ) Now, we regress y y on x x , and let's observe the output to see how good of a fit it is. model = OLS . from_formula ( 'y ~ x' , data = df ) results = model . fit () results . params Intercept 0.084582 x 3.338945 dtype: float64 We're close, but not really there. (Remember, though, we wouldn't know this in a real-world scenario, where we might have postulated the presence of a hidden variable but didn't have the know-how to go out and measure it.) In the real-world scenario above, we might want to see if there's an instrument variable to help us out with this problem. The use of an instrumental variable works as such: We first regress the instrument variable x x on i i , to obtain 'estimated' values of \\delta i \\delta i . We then regress y y on \\delta i \\delta i , which gives us the coefficient alpha alpha . Don't believe me? Look at the math below: As a structured causal equation, the graphical model can be expressed as such: y = \\alpha x + \\gamma h + n_{y}$$ $$x = \\delta i + \\beta h + n_{x} y = \\alpha x + \\gamma h + n_{y}$$ $$x = \\delta i + \\beta h + n_{x} where n_{x} n_{x} is the noise that generates uncertainty around x x , and n_{y} n_{y} is the noise that generates uncertainty around y y . Substuting x x into the y y (don't take my word for it, try it yourself!), we get: y = (\\alpha \\beta + \\gamma) h + (\\alpha) (\\delta i) + \\alpha n_{x} + n_{y} y = (\\alpha \\beta + \\gamma) h + (\\alpha) (\\delta i) + \\alpha n_{x} + n_{y} The parentheses have been rearranged intentionally for the variable i i . If we regress x x on i i , we will get an estimate for the value of \\delta \\delta . By then multiplying \\delta \\delta by i i , we will get \"fitted\" values of i i . We can then regress y y on \\delta i \\delta i to get the value of \\alpha \\alpha , which is exactly what we want! Enough in words, let's look at the code for this!","title":"Model"},{"location":"02-instrument-variables/#mechanics","text":"First, we regress x x on i i . model = OLS . from_formula ( 'x ~ i' , data = df ) results = model . fit () results . params [ 'i' ] -0.9589112040092465 Notice how we get an estimator that is kind of off. It isn't quite accurate, but my gut feeling tells me that's ok. To create the fitted i i , we multiply the learned regression parameter by the original values. df [ '\u03b4i' ] = df [ 'i' ] * results . params [ 'i' ] Then, we regress y y on \\delta i \\delta i : model = OLS . from_formula ( 'y ~ \u03b4i' , data = df ) results = model . fit () results . params Intercept -0.055847 \u03b4i 2.985291 dtype: float64 Voila! We get back the effect of x x on y y by use of this instrument variable i i ! Really happy having seen that it works, and having seen some of the math that goes on behind it!","title":"Mechanics"},{"location":"02-instrument-variables/#assumptions","text":"Now, all of this sounds good and nice, but it does seem a bit \"magical\", to say the least. After all, \"linearity\" does seem like a very strong assumption. Moreover, in order to use an instrument variable, we have to justify that it has: a causal effect on x x , no causal effect on y y , and no causal effect on h h Indeed, there is no free lunch here: we have to use background knowledge (or other means) to justify why i i is a suitable instrument variable; simply asserting this point is not sufficient.","title":"Assumptions"},{"location":"02-instrument-variables/#acknowledgments","text":"I would like to thank Maxmillian Gobel for pointing out linguistic inconsistencies in this notebook and raising them with me.","title":"Acknowledgments"},{"location":"03-d-separation/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); d d -separation d d -separation is a key concept in causal inference. Why is d d -separation important? Looking at this page (by Pearl himself): d d -separation is a criterion for deciding, from a given a causal graph, whether a set X X of variables is independent of another set Y Y , given a third set S S . (I modified that last symbol for consistency here.) % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import networkx as nx from causality_notes import draw_graph To get into d d -separation, we first have to understand paths and blocked paths . One thing that I didn't grok immediately when reading Judea Pearl's book on Causality was that the difference between a path and a directed path . For me, coming in with a fairly shallow network science background, I could not see how \"a path\" could be traced from A A to D D in the following graph: A \\rightarrow B \\leftarrow C \\rightarrow D A \\rightarrow B \\leftarrow C \\rightarrow D By simply traversing the graph from A, we can only ever arrive at B... right? Yes, but only if we accept the traditional \"network science\" definitions of a path. In causal inference, a path is just any undirected connection between variables; a directed path, on the other hand, has to follow the directions on the edges. Therefore, we have to consider the \"undirected\" version of the graph: A - B - C - D A - B - C - D Mechanically, what we are doing when we finding out whether two nodes n_1 n_1 and n_2 n_2 are d d -separated or not is to first start with the undirected version of the causal graph, then find every single path between the two nodes in the undirected graph, then see if there are blockers to independence between n_1 n_1 and n_2 n_2 in the directed version of the graph (as determined by three rules, which I will go through below). This notebook is going to be structured as a hybrid between \"my notes from Jonas Peters' lecture series\" and \"an exercise in implementing algorithms\" related to d d -separation and inferring causal structures from observational data (under certain assumptions). Let's say we have the following causal structure: G = nx . DiGraph () G . add_edge ( 'x2' , 'x1' ) G . add_edge ( 'x3' , 'x1' ) G . add_edge ( 'x4' , 'x3' ) G . add_edge ( 'x4' , 'x5' ) G . add_edge ( 'x1' , 'x5' ) draw_graph ( G ) There are some definitions that we have to get clear with. Path: Convert the graph to an undirected graph. Then ask if there is a connection between the two nodes or not. Directed Path: Follow the arrows! V-structure: An example in the above graph is x_1 x_1 : it has two parents, x_3 x_3 and x_2 x_2 which are not connected by an arrow to each other. From this, we then get to the definition of d d -separation: Two nodes x_i x_i and x_j x_j are d d -separated by the node set S S if all paths between x_i x_i and x_j x_j are blocked by the node set S S . We also call node set S S the \"conditioning set\". There are three rules to determine this. For each node n n in S S , we check whether it looks like the following: \\rightarrow n \\rightarrow \\rightarrow n \\rightarrow , where n n is in the conditioning set S S , \\leftarrow n \\rightarrow \\leftarrow n \\rightarrow , where n n is in the conditioning set S S \\rightarrow n \\leftarrow \\rightarrow n \\leftarrow , where n n is not in the conditioning set S S (these are the v-structures ). There is a supplemental rule: If n n has a descendant that is in S S , and n n is not in S S , then then x_i x_i and x_j x_j are not d d -separated. (recall: don't follow the arrows, as we're not referring to directed paths) draw_graph ( G ) Example 1 Anchoring ourself in the above example, let's ask if x_2 x_2 and x_5 x_5 are d d -separated by the node set S = \\{x_1, x_4\\} S = \\{x_1, x_4\\} . x_1 x_1 lies on the path from x_2 x_2 to x_5 x_5 , and looks like Rule #1. x_4 x_4 lies on the path from x_2 x_2 to x_5 x_5 (the path is x_2 \\rightarrow x_1 \\leftarrow x_3 \\leftarrow x_4 \\rightarrow x_5 x_2 \\rightarrow x_1 \\leftarrow x_3 \\leftarrow x_4 \\rightarrow x_5 ), and looks like Rule #2. Therefore, by rules #1 and #2, \\{x_2, x_5\\} \\{x_2, x_5\\} are d d -separated by S = \\{x_1, x_4\\} S = \\{x_1, x_4\\} . draw_graph ( G ) Example 2 Let's ask if x_1 x_1 and x_4 x_4 are d d -separated by the node set S = \\{x_2, x_3\\} S = \\{x_2, x_3\\} . x_2 x_2 does not lie on a causal path from x_1 x_1 to x_4 x_4 . x_3 x_3 lies on a causal path from x_1 x_1 to x_4 x_4 (the path is x_1 \\leftarrow x_3 \\leftarrow x_4 x_1 \\leftarrow x_3 \\leftarrow x_4 ), and looks like Rule #1. The other path from x_1 x_1 to x_4 x_4 is x_1 \\rightarrow x_5 \\leftarrow x_4 x_1 \\rightarrow x_5 \\leftarrow x_4 , and x_5 x_5 is not in the node set S S , therefore this looks like Rule #3. Therefore, by rules #1 and #3, \\{x_1, x_4\\} \\{x_1, x_4\\} are d d -separated by S = \\{x_2, x_3\\} S = \\{x_2, x_3\\} . draw_graph ( G ) Example 3 Finally, let's ask if x_2 x_2 and x_4 x_4 are d d -separated by the node set S = \\{\\} S = \\{\\} . There are two sets of paths to go from x_2 x_2 to x_4 x_4 : x_2 \\rightarrow x_1 \\rightarrow x_5 \\leftarrow x_4 x_2 \\rightarrow x_1 \\rightarrow x_5 \\leftarrow x_4 x_2 \\rightarrow x_1 \\leftarrow x_3 \\leftarrow x_4 x_2 \\rightarrow x_1 \\leftarrow x_3 \\leftarrow x_4 In both cases, x_1 x_1 is not in node set S=\\{\\} S=\\{\\} , and looks like Rule #3. Therefore, by rule #3, x_2 x_2 and x_4 x_4 are d d -separated by S = \\{\\} S = \\{\\} . Algorithm From the above examples, I think I have a way of writing an algorithm that can automatically check for d d -separation. Firstly, we have to define the three rules as functions. from causality_notes import rule1 , rule2 , rule3 # rule1?? # rule2?? # rule3?? Then, we define the d d -separation algorithm. Read through the code and comments to learn what's going on! def d_separation ( n1 , n2 , S , G : nx . DiGraph ): \"\"\" Checks whether nodes n1 and n2 are d-separated by the set S. :param n1: A node in the graph G. :param n2: A node in the graph G. :param S: The conditioning set of interest. :param G: A NetworkX directed graph. :returns: (bool) dsep. \"\"\" # Defensive programming checks. def error ( n ): \"\"\"Custom error message for assertions below.\"\"\" return f \"node { n } not in graph G\" assert n1 in G . nodes (), error ( n1 ) assert n2 in G . nodes (), error ( n2 ) for n in S : assert n in G . nodes (), error ( n ) # First, we hold an undirected copy of the graph. Gpath = G . to_undirected () # Next, we check whether there is a path from node n1 to n2. assert nx . has_path ( Gpath , n1 , n2 ) # Next, we iterate over each path between n1 and n2, and check for the three rules. # # Any one of the three rules has to be fulfilled on a path for the path to be # blocked by the set S. # # However, blocking must occur on all paths, otherwise, the two nodes n1 and n2 are # not d-separated. paths_blocked = [] for path in nx . all_simple_paths ( G . to_undirected (), n1 , n2 ): is_blocked = False for node in path : if node is not n1 and node is not n2 : pass1 = rule1 ( node , S , G , path ) pass2 = rule2 ( node , S , G , path ) pass3 = rule3 ( node , S , G , path ) if ( pass1 or pass2 or pass3 ): is_blocked = True paths_blocked . append ( is_blocked ) return all ( paths_blocked ) Finally, let's run the test cases. From the examples above, x_2 x_2 and x_5 x_5 are d d -separated by \\{x_1, x_4\\} \\{x_1, x_4\\} : d_separation ( 'x2' , 'x5' , set ([ 'x1' , 'x4' ]), G ) True Also, x_1 x_1 and x_4 x_4 are d d -separated by \\{x_2, x_3\\} \\{x_2, x_3\\} : d_separation ( 'x1' , 'x4' , set ([ 'x2' , 'x3' ]), G ) True Finally, x_2 x_2 and x_4 x_4 are d d -separated by \\{\\} \\{\\} (an empty set of nodes): d_separation ( 'x2' , 'x4' , set ([]), G ) True Woohoo! The hard part about doing this manually is that it's difficult to manually enumerate all simple paths between two nodes on a graph. Like, tracing it and keeping it in memory is difficult. But implementing the rules as an algorithm helps. A few more tests: Edges should not be d d -separated. for n1 , n2 in G . edges (): assert not d_separation ( n1 , n2 , set ([]), G ) Example 4 Let's try a different causal graph, G2 G2 , which is part of Example 3 in Pearl's d d -separation without tears . import matplotlib.pyplot as plt G2 = nx . DiGraph () edges = [ 'xr' , 'rw' , 'rs' , 'st' , 'tp' , 'ut' , 'vu' , 'vq' , 'vy' ] edges = [( f ' { i [ 0 ] } ' , f ' { i [ 1 ] } ' ) for i in edges ] G2 . add_edges_from ( edges ) fig = plt . figure () ax = fig . add_subplot ( 111 ) pos = { 'x' : ( 1 , 0 ), 'r' : ( 2 , 0 ), 's' : ( 3 , 0 ), 't' : ( 4 , 0 ), 'u' : ( 5 , 0 ), 'v' : ( 6 , 0 ), 'y' : ( 7 , 0 ), 'w' : ( 2 , - 1 ), 'p' : ( 4 , - 1 ), 'q' : ( 6 , - 1 )} # pos = nx.spring_layout(G2) nx . draw ( G2 , pos = pos , with_labels = True , ax = ax ) ax . set_aspect ( 'equal' ) ax . set_ylim ( - 2 , 1 ) ax . set_xlim ( - 1 , 8 ) plt . tight_layout () In Pearl's page, he sets up a hypothetical regression of y y on p p , r r and x x : y = c_1p + c_2r + c_3x y = c_1p + c_2r + c_3x A priori , it is possible to know which regression coefficient is going to be zero, if we know the causal graph and assume that the relationship is linear. To check whether c_3 c_3 will be zero, we ask whether y y and x x are d d -separated by \\{p, r\\} \\{p, r\\} : d_separation ( 'x' , 'y' , set ([ 'r' , 'p' ]), G2 ) True To check whether c_1 c_1 will be zero, we ask whether y y and p p are d d -separated by \\{r, x\\} \\{r, x\\} : d_separation ( 'p' , 'y' , set ([ 'x' , 'r' ]), G2 ) False To check whether c_2 c_2 will be zero, we ask whether y y and r r are d d -separated by \\{x, p\\} \\{x, p\\} : d_separation ( 'r' , 'y' , set ([ 'x' , 'p' ]), G2 ) False y y and r r are not d d -separated (i.e. they are d d -connected), because t t , which is a collider (and would originally have blocked the path), has a descendant p p that is part of the conditioning set. So, why is this important? It allows us to state a thing called the \"Markov condition\": The joint probability distribution P P between two variables x_i x_i and x_j x_j is Markov w.r.t. the graph G G if, for the conditioning set S S : x_i x_i and x_j x_j are d d -separated by S S in G G . \\implies \\implies (implies) x_i x_i is conditionally independent of x_j x_j , conditioned on S S .","title":"03: d-Separation"},{"location":"03-d-separation/#dd-separation","text":"d d -separation is a key concept in causal inference. Why is d d -separation important? Looking at this page (by Pearl himself): d d -separation is a criterion for deciding, from a given a causal graph, whether a set X X of variables is independent of another set Y Y , given a third set S S . (I modified that last symbol for consistency here.) % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import networkx as nx from causality_notes import draw_graph To get into d d -separation, we first have to understand paths and blocked paths . One thing that I didn't grok immediately when reading Judea Pearl's book on Causality was that the difference between a path and a directed path . For me, coming in with a fairly shallow network science background, I could not see how \"a path\" could be traced from A A to D D in the following graph: A \\rightarrow B \\leftarrow C \\rightarrow D A \\rightarrow B \\leftarrow C \\rightarrow D By simply traversing the graph from A, we can only ever arrive at B... right? Yes, but only if we accept the traditional \"network science\" definitions of a path. In causal inference, a path is just any undirected connection between variables; a directed path, on the other hand, has to follow the directions on the edges. Therefore, we have to consider the \"undirected\" version of the graph: A - B - C - D A - B - C - D Mechanically, what we are doing when we finding out whether two nodes n_1 n_1 and n_2 n_2 are d d -separated or not is to first start with the undirected version of the causal graph, then find every single path between the two nodes in the undirected graph, then see if there are blockers to independence between n_1 n_1 and n_2 n_2 in the directed version of the graph (as determined by three rules, which I will go through below). This notebook is going to be structured as a hybrid between \"my notes from Jonas Peters' lecture series\" and \"an exercise in implementing algorithms\" related to d d -separation and inferring causal structures from observational data (under certain assumptions). Let's say we have the following causal structure: G = nx . DiGraph () G . add_edge ( 'x2' , 'x1' ) G . add_edge ( 'x3' , 'x1' ) G . add_edge ( 'x4' , 'x3' ) G . add_edge ( 'x4' , 'x5' ) G . add_edge ( 'x1' , 'x5' ) draw_graph ( G ) There are some definitions that we have to get clear with. Path: Convert the graph to an undirected graph. Then ask if there is a connection between the two nodes or not. Directed Path: Follow the arrows! V-structure: An example in the above graph is x_1 x_1 : it has two parents, x_3 x_3 and x_2 x_2 which are not connected by an arrow to each other. From this, we then get to the definition of d d -separation: Two nodes x_i x_i and x_j x_j are d d -separated by the node set S S if all paths between x_i x_i and x_j x_j are blocked by the node set S S . We also call node set S S the \"conditioning set\". There are three rules to determine this. For each node n n in S S , we check whether it looks like the following: \\rightarrow n \\rightarrow \\rightarrow n \\rightarrow , where n n is in the conditioning set S S , \\leftarrow n \\rightarrow \\leftarrow n \\rightarrow , where n n is in the conditioning set S S \\rightarrow n \\leftarrow \\rightarrow n \\leftarrow , where n n is not in the conditioning set S S (these are the v-structures ). There is a supplemental rule: If n n has a descendant that is in S S , and n n is not in S S , then then x_i x_i and x_j x_j are not d d -separated. (recall: don't follow the arrows, as we're not referring to directed paths) draw_graph ( G )","title":"dd-separation"},{"location":"03-d-separation/#example-1","text":"Anchoring ourself in the above example, let's ask if x_2 x_2 and x_5 x_5 are d d -separated by the node set S = \\{x_1, x_4\\} S = \\{x_1, x_4\\} . x_1 x_1 lies on the path from x_2 x_2 to x_5 x_5 , and looks like Rule #1. x_4 x_4 lies on the path from x_2 x_2 to x_5 x_5 (the path is x_2 \\rightarrow x_1 \\leftarrow x_3 \\leftarrow x_4 \\rightarrow x_5 x_2 \\rightarrow x_1 \\leftarrow x_3 \\leftarrow x_4 \\rightarrow x_5 ), and looks like Rule #2. Therefore, by rules #1 and #2, \\{x_2, x_5\\} \\{x_2, x_5\\} are d d -separated by S = \\{x_1, x_4\\} S = \\{x_1, x_4\\} . draw_graph ( G )","title":"Example 1"},{"location":"03-d-separation/#example-2","text":"Let's ask if x_1 x_1 and x_4 x_4 are d d -separated by the node set S = \\{x_2, x_3\\} S = \\{x_2, x_3\\} . x_2 x_2 does not lie on a causal path from x_1 x_1 to x_4 x_4 . x_3 x_3 lies on a causal path from x_1 x_1 to x_4 x_4 (the path is x_1 \\leftarrow x_3 \\leftarrow x_4 x_1 \\leftarrow x_3 \\leftarrow x_4 ), and looks like Rule #1. The other path from x_1 x_1 to x_4 x_4 is x_1 \\rightarrow x_5 \\leftarrow x_4 x_1 \\rightarrow x_5 \\leftarrow x_4 , and x_5 x_5 is not in the node set S S , therefore this looks like Rule #3. Therefore, by rules #1 and #3, \\{x_1, x_4\\} \\{x_1, x_4\\} are d d -separated by S = \\{x_2, x_3\\} S = \\{x_2, x_3\\} . draw_graph ( G )","title":"Example 2"},{"location":"03-d-separation/#example-3","text":"Finally, let's ask if x_2 x_2 and x_4 x_4 are d d -separated by the node set S = \\{\\} S = \\{\\} . There are two sets of paths to go from x_2 x_2 to x_4 x_4 : x_2 \\rightarrow x_1 \\rightarrow x_5 \\leftarrow x_4 x_2 \\rightarrow x_1 \\rightarrow x_5 \\leftarrow x_4 x_2 \\rightarrow x_1 \\leftarrow x_3 \\leftarrow x_4 x_2 \\rightarrow x_1 \\leftarrow x_3 \\leftarrow x_4 In both cases, x_1 x_1 is not in node set S=\\{\\} S=\\{\\} , and looks like Rule #3. Therefore, by rule #3, x_2 x_2 and x_4 x_4 are d d -separated by S = \\{\\} S = \\{\\} .","title":"Example 3"},{"location":"03-d-separation/#algorithm","text":"From the above examples, I think I have a way of writing an algorithm that can automatically check for d d -separation. Firstly, we have to define the three rules as functions. from causality_notes import rule1 , rule2 , rule3 # rule1?? # rule2?? # rule3?? Then, we define the d d -separation algorithm. Read through the code and comments to learn what's going on! def d_separation ( n1 , n2 , S , G : nx . DiGraph ): \"\"\" Checks whether nodes n1 and n2 are d-separated by the set S. :param n1: A node in the graph G. :param n2: A node in the graph G. :param S: The conditioning set of interest. :param G: A NetworkX directed graph. :returns: (bool) dsep. \"\"\" # Defensive programming checks. def error ( n ): \"\"\"Custom error message for assertions below.\"\"\" return f \"node { n } not in graph G\" assert n1 in G . nodes (), error ( n1 ) assert n2 in G . nodes (), error ( n2 ) for n in S : assert n in G . nodes (), error ( n ) # First, we hold an undirected copy of the graph. Gpath = G . to_undirected () # Next, we check whether there is a path from node n1 to n2. assert nx . has_path ( Gpath , n1 , n2 ) # Next, we iterate over each path between n1 and n2, and check for the three rules. # # Any one of the three rules has to be fulfilled on a path for the path to be # blocked by the set S. # # However, blocking must occur on all paths, otherwise, the two nodes n1 and n2 are # not d-separated. paths_blocked = [] for path in nx . all_simple_paths ( G . to_undirected (), n1 , n2 ): is_blocked = False for node in path : if node is not n1 and node is not n2 : pass1 = rule1 ( node , S , G , path ) pass2 = rule2 ( node , S , G , path ) pass3 = rule3 ( node , S , G , path ) if ( pass1 or pass2 or pass3 ): is_blocked = True paths_blocked . append ( is_blocked ) return all ( paths_blocked ) Finally, let's run the test cases. From the examples above, x_2 x_2 and x_5 x_5 are d d -separated by \\{x_1, x_4\\} \\{x_1, x_4\\} : d_separation ( 'x2' , 'x5' , set ([ 'x1' , 'x4' ]), G ) True Also, x_1 x_1 and x_4 x_4 are d d -separated by \\{x_2, x_3\\} \\{x_2, x_3\\} : d_separation ( 'x1' , 'x4' , set ([ 'x2' , 'x3' ]), G ) True Finally, x_2 x_2 and x_4 x_4 are d d -separated by \\{\\} \\{\\} (an empty set of nodes): d_separation ( 'x2' , 'x4' , set ([]), G ) True Woohoo! The hard part about doing this manually is that it's difficult to manually enumerate all simple paths between two nodes on a graph. Like, tracing it and keeping it in memory is difficult. But implementing the rules as an algorithm helps. A few more tests: Edges should not be d d -separated. for n1 , n2 in G . edges (): assert not d_separation ( n1 , n2 , set ([]), G )","title":"Algorithm"},{"location":"03-d-separation/#example-4","text":"Let's try a different causal graph, G2 G2 , which is part of Example 3 in Pearl's d d -separation without tears . import matplotlib.pyplot as plt G2 = nx . DiGraph () edges = [ 'xr' , 'rw' , 'rs' , 'st' , 'tp' , 'ut' , 'vu' , 'vq' , 'vy' ] edges = [( f ' { i [ 0 ] } ' , f ' { i [ 1 ] } ' ) for i in edges ] G2 . add_edges_from ( edges ) fig = plt . figure () ax = fig . add_subplot ( 111 ) pos = { 'x' : ( 1 , 0 ), 'r' : ( 2 , 0 ), 's' : ( 3 , 0 ), 't' : ( 4 , 0 ), 'u' : ( 5 , 0 ), 'v' : ( 6 , 0 ), 'y' : ( 7 , 0 ), 'w' : ( 2 , - 1 ), 'p' : ( 4 , - 1 ), 'q' : ( 6 , - 1 )} # pos = nx.spring_layout(G2) nx . draw ( G2 , pos = pos , with_labels = True , ax = ax ) ax . set_aspect ( 'equal' ) ax . set_ylim ( - 2 , 1 ) ax . set_xlim ( - 1 , 8 ) plt . tight_layout () In Pearl's page, he sets up a hypothetical regression of y y on p p , r r and x x : y = c_1p + c_2r + c_3x y = c_1p + c_2r + c_3x A priori , it is possible to know which regression coefficient is going to be zero, if we know the causal graph and assume that the relationship is linear. To check whether c_3 c_3 will be zero, we ask whether y y and x x are d d -separated by \\{p, r\\} \\{p, r\\} : d_separation ( 'x' , 'y' , set ([ 'r' , 'p' ]), G2 ) True To check whether c_1 c_1 will be zero, we ask whether y y and p p are d d -separated by \\{r, x\\} \\{r, x\\} : d_separation ( 'p' , 'y' , set ([ 'x' , 'r' ]), G2 ) False To check whether c_2 c_2 will be zero, we ask whether y y and r r are d d -separated by \\{x, p\\} \\{x, p\\} : d_separation ( 'r' , 'y' , set ([ 'x' , 'p' ]), G2 ) False y y and r r are not d d -separated (i.e. they are d d -connected), because t t , which is a collider (and would originally have blocked the path), has a descendant p p that is part of the conditioning set. So, why is this important? It allows us to state a thing called the \"Markov condition\": The joint probability distribution P P between two variables x_i x_i and x_j x_j is Markov w.r.t. the graph G G if, for the conditioning set S S : x_i x_i and x_j x_j are d d -separated by S S in G G . \\implies \\implies (implies) x_i x_i is conditionally independent of x_j x_j , conditioned on S S .","title":"Example 4"},{"location":"04-finding-confounding-set/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction Following up on d d -separation, my colleagues and I chatted about how to find the confounding set of variables in a causal graph. This is another graph search problem. Let's see how this can be applied. from causality_notes import rule1 , rule2 , rule3 , path_nodes import networkx as nx import matplotlib.pyplot as plt % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' From Judea Pearl's book, there is a diagram in chapter 4, Figure 4.7 . Let's reproduce it here. G = nx . DiGraph () edges = [ ( 'D' , 'A' ), ( 'D' , 'C' ), ( 'F' , 'C' ), ( 'A' , 'B' ), ( 'C' , 'B' ), ( 'C' , 'Y' ), ( 'F' , 'X' ), ( 'F' , 'Y' ), ( 'C' , 'E' ), ( 'A' , 'X' ), ( 'E' , 'X' ), ( 'E' , 'Y' ), ( 'B' , 'X' ), ( 'X' , 'Y' ), ( 'G' , 'X' ), ( 'G' , 'Y' ) ] G . add_edges_from ( edges ) pos = { 'D' : ( 0 , 0 ), 'A' : ( 1 , 0.5 ), 'C' : ( 1 , - 1 ), 'F' : ( 1 , - 2 ), 'B' : ( 2 , - 0.3 ), 'E' : ( 2 , 1 ), 'X' : ( 4 , 0.5 ), 'G' : ( 4.5 , - 2 ), 'Y' : ( 5 , 0.5 ) } nx . draw ( G , pos = pos , with_labels = True ) To reveal the answer, the minimum confounding set is \\{A, B, E, F, G\\} \\{A, B, E, F, G\\} . What we would like to know is what is the set of confounders that we need to control for in order to correctly estimate the effect of X X on Y Y . To do this, we use the following logic: Find all undirected paths between X X and Y Y . Traverse each node in the undirected paths. Check to see if, in the directed graph, the node blocks the path between X X and Y Y if it were in the conditioning set. If yes, then it should be included as a confounder. Break out and continue on to next path. If no, it should not be included as a confounder. Gpath = G . to_undirected () confounders = set () n1 = 'X' n2 = 'Y' for i , path in enumerate ( nx . all_simple_paths ( Gpath , n1 , n2 )): for n in path : if n is not n1 and n is not n2 : pass1 = rule1 ( n , [ n ], G , path ) pass2 = rule2 ( n , [ n ], G , path ) pass3 = rule3 ( n , [], G , path ) if pass1 or pass2 or pass3 : confounders . add ( n ) # We break, because as soon as we find a good # blocking node, there is no need to continue # looking at other nodes. break confounders {'A', 'B', 'E', 'F', 'G'} We did it!","title":"04: Finding Confounding Set"},{"location":"04-finding-confounding-set/#introduction","text":"Following up on d d -separation, my colleagues and I chatted about how to find the confounding set of variables in a causal graph. This is another graph search problem. Let's see how this can be applied. from causality_notes import rule1 , rule2 , rule3 , path_nodes import networkx as nx import matplotlib.pyplot as plt % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' From Judea Pearl's book, there is a diagram in chapter 4, Figure 4.7 . Let's reproduce it here. G = nx . DiGraph () edges = [ ( 'D' , 'A' ), ( 'D' , 'C' ), ( 'F' , 'C' ), ( 'A' , 'B' ), ( 'C' , 'B' ), ( 'C' , 'Y' ), ( 'F' , 'X' ), ( 'F' , 'Y' ), ( 'C' , 'E' ), ( 'A' , 'X' ), ( 'E' , 'X' ), ( 'E' , 'Y' ), ( 'B' , 'X' ), ( 'X' , 'Y' ), ( 'G' , 'X' ), ( 'G' , 'Y' ) ] G . add_edges_from ( edges ) pos = { 'D' : ( 0 , 0 ), 'A' : ( 1 , 0.5 ), 'C' : ( 1 , - 1 ), 'F' : ( 1 , - 2 ), 'B' : ( 2 , - 0.3 ), 'E' : ( 2 , 1 ), 'X' : ( 4 , 0.5 ), 'G' : ( 4.5 , - 2 ), 'Y' : ( 5 , 0.5 ) } nx . draw ( G , pos = pos , with_labels = True ) To reveal the answer, the minimum confounding set is \\{A, B, E, F, G\\} \\{A, B, E, F, G\\} . What we would like to know is what is the set of confounders that we need to control for in order to correctly estimate the effect of X X on Y Y . To do this, we use the following logic: Find all undirected paths between X X and Y Y . Traverse each node in the undirected paths. Check to see if, in the directed graph, the node blocks the path between X X and Y Y if it were in the conditioning set. If yes, then it should be included as a confounder. Break out and continue on to next path. If no, it should not be included as a confounder. Gpath = G . to_undirected () confounders = set () n1 = 'X' n2 = 'Y' for i , path in enumerate ( nx . all_simple_paths ( Gpath , n1 , n2 )): for n in path : if n is not n1 and n is not n2 : pass1 = rule1 ( n , [ n ], G , path ) pass2 = rule2 ( n , [ n ], G , path ) pass3 = rule3 ( n , [], G , path ) if pass1 or pass2 or pass3 : confounders . add ( n ) # We break, because as soon as we find a good # blocking node, there is no need to continue # looking at other nodes. break confounders {'A', 'B', 'E', 'F', 'G'} We did it!","title":"Introduction"},{"location":"05-collider-effect/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction In this notebook, we will take a quick look at the \"collider\" effect. Let's say we have the following causal graph: a \\rightarrow b \\leftarrow c a \\rightarrow b \\leftarrow c Apparently, if we \"condition\" on b b , then a a and c c will be correlated, even though they are independent. import numpy as np from causality_notes import noise import pandas as pd import seaborn as sns % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Generate Data Let's assume we have a causal model that follows the equations below: a \\sim N(0, 1)$$ $$c \\sim N(0, 1)$$ $$b = 20a - 20c a \\sim N(0, 1)$$ $$c \\sim N(0, 1)$$ $$b = 20a - 20c This is expressed in the code below. size = 1000 a = noise ( size ) c = noise ( size ) b = 20 * a - 20 * c + noise ( size ) We now make it into a pandas DataFrame. df = pd . DataFrame ({ 'a' : a , 'b' : b , 'c' : c }) Let's view a pair plot to see the pairwise correlation (dependency) between the variables. sns . pairplot ( df ) <seaborn.axisgrid.PairGrid at 0x7f3e54047ee0> Ok, as shown in the causal graph, a a and c c are independent of one another, and so distributionally, there's no trend between them. Conditioning When we \"condition\" on a variable, remember that we are essentially taking a \"slice\" of a variable, and seeing what the distributions for the other variables are. I illustrated this on my blog . In our problem, this means that we have to slice out a range of the values of b b : df_new = df [( df [ 'b' ] < df [ 'b' ] . mean ()) & ( df [ 'b' ] > np . percentile ( df [ 'b' ], 25 ))] Now, let's visualize the relationship between a a and c c , now conditioned on b b . ax = df_new . plot ( kind = 'scatter' , x = 'a' , y = 'c' ) ax . set_aspect ( 'equal' ) ax . set_title ( 'conditioned on b' ) Text(0.5, 1.0, 'conditioned on b') We can also look at the full joint distribution of a a and c c , colouring b b to illustrate what would happen if we conditioned on particular values of b b . ax = sns . scatterplot ( data = df , x = 'a' , y = 'c' , hue = 'b' ) ax . set_aspect ( 'equal' ) Conclusion Here, we see that in a collider situation, if we condition on the child variable, the parents will be unduly correlated.","title":"05: Collider Effect"},{"location":"05-collider-effect/#introduction","text":"In this notebook, we will take a quick look at the \"collider\" effect. Let's say we have the following causal graph: a \\rightarrow b \\leftarrow c a \\rightarrow b \\leftarrow c Apparently, if we \"condition\" on b b , then a a and c c will be correlated, even though they are independent. import numpy as np from causality_notes import noise import pandas as pd import seaborn as sns % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina'","title":"Introduction"},{"location":"05-collider-effect/#generate-data","text":"Let's assume we have a causal model that follows the equations below: a \\sim N(0, 1)$$ $$c \\sim N(0, 1)$$ $$b = 20a - 20c a \\sim N(0, 1)$$ $$c \\sim N(0, 1)$$ $$b = 20a - 20c This is expressed in the code below. size = 1000 a = noise ( size ) c = noise ( size ) b = 20 * a - 20 * c + noise ( size ) We now make it into a pandas DataFrame. df = pd . DataFrame ({ 'a' : a , 'b' : b , 'c' : c }) Let's view a pair plot to see the pairwise correlation (dependency) between the variables. sns . pairplot ( df ) <seaborn.axisgrid.PairGrid at 0x7f3e54047ee0> Ok, as shown in the causal graph, a a and c c are independent of one another, and so distributionally, there's no trend between them.","title":"Generate Data"},{"location":"05-collider-effect/#conditioning","text":"When we \"condition\" on a variable, remember that we are essentially taking a \"slice\" of a variable, and seeing what the distributions for the other variables are. I illustrated this on my blog . In our problem, this means that we have to slice out a range of the values of b b : df_new = df [( df [ 'b' ] < df [ 'b' ] . mean ()) & ( df [ 'b' ] > np . percentile ( df [ 'b' ], 25 ))] Now, let's visualize the relationship between a a and c c , now conditioned on b b . ax = df_new . plot ( kind = 'scatter' , x = 'a' , y = 'c' ) ax . set_aspect ( 'equal' ) ax . set_title ( 'conditioned on b' ) Text(0.5, 1.0, 'conditioned on b') We can also look at the full joint distribution of a a and c c , colouring b b to illustrate what would happen if we conditioned on particular values of b b . ax = sns . scatterplot ( data = df , x = 'a' , y = 'c' , hue = 'b' ) ax . set_aspect ( 'equal' )","title":"Conditioning"},{"location":"05-collider-effect/#conclusion","text":"Here, we see that in a collider situation, if we condition on the child variable, the parents will be unduly correlated.","title":"Conclusion"},{"location":"06-causality-identifiability/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Directional Identifiability From Jonas Peters' lecture 3 on causality at the Broad. If we have a presumed causal model of the linear form: Y = \\alpha X + N_y Y = \\alpha X + N_y where N_y N_y is i.i.d. noise in Y Y , and X X and N_y N_y are both independent and non-Gaussian, then we cannot find X = \\beta Y + N_x X = \\beta Y + N_x where N_x N_x is i.i.d. noise in X X that also satisfies the independence constraints. In simpler words, if we assume that the distributions of X X and N_y N_y are non-Gaussian, then we will know that the causal model goes from X \\rightarrow Y X \\rightarrow Y and not Y \\rightarrow X Y \\rightarrow X . Let's simulate this. import numpy as np import matplotlib.pyplot as plt % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Firstly, we will generate non-Gaussian Xs and Ys. X_ng = np . random . uniform ( - 1 , 1 , size = 1000 ) alpha = 2 N_y_ng = np . random . uniform ( - 0.4 , 0.4 , size = 1000 ) y_ng = alpha * X_ng + N_y_ng Now, let's plot Y against X. plt . scatter ( X_ng , y_ng ) plt . ylabel ( \"Y\" ) plt . xlabel ( \"X\" ) plt . show () Now, let's also simulate the case where X X and N_y N_y are Gaussian-distributed and independent. X_g = np . random . normal ( 0 , 0.5 , size = 1000 ) alpha_g = 2 N_y_g = np . random . normal ( 0 , 1 , size = 1000 ) y_g = alpha_g * X_g + N_y_g plt . scatter ( X_g , y_g ) plt . xlabel ( \"X\" ) plt . ylabel ( \"Y\" ) plt . show () We will now fit X as a function of Y, and do a residual analysis to see whether our residuals (i.e. noise) are independent of the input (in this case Y). Remember, we are looking to check that the inverse condition holds. from sklearn.linear_model import LinearRegression lm = LinearRegression () Firstly, we X as function of Y and obtain a coefficient. lm . fit ( y_g . reshape ( - 1 , 1 ), X_g ) coeff_g = lm . coef_ We then do the same for the non-Gaussian case. lm . fit ( y_ng . reshape ( - 1 , 1 ), X_ng ) coeff_ng = lm . coef_ Great! Now that we have the coefficients out, let's move on to the analysis of residuals. We will be checking that the residuals ( X - \\beta Y X - \\beta Y ) should be independent of Y Y . First off, the Gaussian case. residuals_g = X_g - coeff_g * y_g plt . scatter ( y_g , residuals_g ) plt . xlabel ( \"Y\" ) plt . ylabel ( \"residual\" ) plt . title ( \"Gaussian\" ) plt . show () We see that there is no trend in the residuals. Now, the non-gaussian case. residuals_ng = X_ng - coeff_ng * y_ng plt . scatter ( y_ng , residuals_ng ) plt . xlabel ( \"Y\" ) plt . ylabel ( \"residuals\" ) plt . title ( \"non-Gaussian\" ) plt . show () We see that there is a clear trend - residual depends on the value of y in the non-Gaussian case, whereas it does not in the Gaussian case. This empirical simulation illustrates how we cannot recover an inverse model where the noise in X ( N_x N_x ) is independent of the value of Y Y . Hence, we have an identifiable model under non-Gaussian assumptions.","title":"06: Causal Identifiability"},{"location":"06-causality-identifiability/#directional-identifiability","text":"From Jonas Peters' lecture 3 on causality at the Broad. If we have a presumed causal model of the linear form: Y = \\alpha X + N_y Y = \\alpha X + N_y where N_y N_y is i.i.d. noise in Y Y , and X X and N_y N_y are both independent and non-Gaussian, then we cannot find X = \\beta Y + N_x X = \\beta Y + N_x where N_x N_x is i.i.d. noise in X X that also satisfies the independence constraints. In simpler words, if we assume that the distributions of X X and N_y N_y are non-Gaussian, then we will know that the causal model goes from X \\rightarrow Y X \\rightarrow Y and not Y \\rightarrow X Y \\rightarrow X . Let's simulate this. import numpy as np import matplotlib.pyplot as plt % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Firstly, we will generate non-Gaussian Xs and Ys. X_ng = np . random . uniform ( - 1 , 1 , size = 1000 ) alpha = 2 N_y_ng = np . random . uniform ( - 0.4 , 0.4 , size = 1000 ) y_ng = alpha * X_ng + N_y_ng Now, let's plot Y against X. plt . scatter ( X_ng , y_ng ) plt . ylabel ( \"Y\" ) plt . xlabel ( \"X\" ) plt . show () Now, let's also simulate the case where X X and N_y N_y are Gaussian-distributed and independent. X_g = np . random . normal ( 0 , 0.5 , size = 1000 ) alpha_g = 2 N_y_g = np . random . normal ( 0 , 1 , size = 1000 ) y_g = alpha_g * X_g + N_y_g plt . scatter ( X_g , y_g ) plt . xlabel ( \"X\" ) plt . ylabel ( \"Y\" ) plt . show () We will now fit X as a function of Y, and do a residual analysis to see whether our residuals (i.e. noise) are independent of the input (in this case Y). Remember, we are looking to check that the inverse condition holds. from sklearn.linear_model import LinearRegression lm = LinearRegression () Firstly, we X as function of Y and obtain a coefficient. lm . fit ( y_g . reshape ( - 1 , 1 ), X_g ) coeff_g = lm . coef_ We then do the same for the non-Gaussian case. lm . fit ( y_ng . reshape ( - 1 , 1 ), X_ng ) coeff_ng = lm . coef_ Great! Now that we have the coefficients out, let's move on to the analysis of residuals. We will be checking that the residuals ( X - \\beta Y X - \\beta Y ) should be independent of Y Y . First off, the Gaussian case. residuals_g = X_g - coeff_g * y_g plt . scatter ( y_g , residuals_g ) plt . xlabel ( \"Y\" ) plt . ylabel ( \"residual\" ) plt . title ( \"Gaussian\" ) plt . show () We see that there is no trend in the residuals. Now, the non-gaussian case. residuals_ng = X_ng - coeff_ng * y_ng plt . scatter ( y_ng , residuals_ng ) plt . xlabel ( \"Y\" ) plt . ylabel ( \"residuals\" ) plt . title ( \"non-Gaussian\" ) plt . show () We see that there is a clear trend - residual depends on the value of y in the non-Gaussian case, whereas it does not in the Gaussian case. This empirical simulation illustrates how we cannot recover an inverse model where the noise in X ( N_x N_x ) is independent of the value of Y Y . Hence, we have an identifiable model under non-Gaussian assumptions.","title":"Directional Identifiability"},{"location":"07-do-operator/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); The Do-Operator and Bayesian Models The do-operator is a foundational idea in causal methods, used to express, in precise mathematical language, what counterfactual interventions would look like. The do-operator from causal inference has a tight connection to probabilistic modelling. But what exactly is that connection? Having myself been previously confused about the link between graphical models, causal structure, and more, here's my current understanding of the connection. What I hope you'll see here is that the do-operator gives us the ability to simulate what an experiment might look like... without actually doing the experiment, and that at its implementation core, it ends up being nothing more than \"set this thing to a particular value\". If you're curious now, it's time to read on :). Causal models and Bayesian models, in brief Let\u2019s start first with a brief overview of the connection between causal models and Bayesian models. In doing my own study on causality , it soon became clear to me that causal models can be read off from mathematical equations quite easily. For a moment, let's assume we have the following set of equations that we presume to describe some data that we observe: b = \\delta d + \\epsilon e + \\sigma_b b = \\delta d + \\epsilon e + \\sigma_b and a = \\beta b + \\gamma c + \\sigma_a a = \\beta b + \\gamma c + \\sigma_a My convention here is that the English letters represent data, while the Greek letters represent parameters of the model. To make this abstract example a bit more concrete, let's connect these equations to something in real life, such as scholastic achievement. In this highly contrived and fictitious example, let's say that a a is the scholastic achievement of a student as measured by the sum total of subject scores across 11 subjects in the O-level examinations administered by the University of Cambridge, and it is thought to be a function of: b b , the intelligence quotient (IQ) of the student, c c , the cost of a student's tuition fees in thousands of dollars (private vs. public school), and \\sigma_a \\sigma_a , the intrinsic variation in student performance as a whole For the term b b , we think it is a function of: d d , the crime rate of their neighborhood, e e , the household income of that student, in thousands of dollars, and \\sigma_b \\sigma_b , the intrinsic variation in IQ. The astute reader will immediately see the controversy in this example. The causal path we assume here isn't going to be the only plausible model, and there are many, many variables we're leaving out here. However, I'd ask that you let your mind suspend the controversy for a moment; it's always a big brain teaser to try to come up with plausible but fictitious examples. To read off the causal model from this set of equations, everything on the right is causal for the stuff on the left. By those equations, we state that the values of a a that we observe are caused by values of b b and c c and their Greek coefficients \\beta \\beta and \\gamma \\gamma plus some noise \\sigma_a \\sigma_a , while the values of b b that we observe are caused by the values of d d and e e and their Greek coefficients \\delta \\delta and \\epsilon \\epsilon plus some noise \\sigma_b \\sigma_b . With Bayesian inference on this model, our task here is to estimate the coefficients , \\beta \\beta , \\gamma \\gamma , \\delta \\delta , and \\epsilon \\epsilon . More precisely, we are interested in estimating their expected value and uncertainty. In order to express the model in such a way that reflects its probabilistic in nature, we need some syntax to express the idea that because \\delta \\delta , \\epsilon \\epsilon and \\sigma_b \\sigma_b are uncertain and are modelled by a probability distribution, therefore b b is uncertain and has a corresponding probability distribution too. At the same time, we need an analogous syntax to express that because \\beta \\beta , \\gamma \\gamma , and \\sigma_a \\sigma_a are uncertain and are modelled by a probability distribution, therefore a a is also uncertain and has a corresponding probability distribution too. Moreover, if we assume that c c , d d , e e are stochastic because they are drawn from a distribution, then we have ~~a classic case where everything is unknown and we can\u2019t do anything~~ an awesome modelling problem at hand! \ud83d\ude42 In any case, for the first equation, our expression for the distribution of b b conditioned on everything on the right would look something like: P(b | \\delta, \\epsilon, \\sigma_b, d, e) P(b | \\delta, \\epsilon, \\sigma_b, d, e) And likewise, the distribution of a a conditioned on everything on the right would look something like this: P(a | \\beta, \\gamma, \\sigma_a, b, c) P(a | \\beta, \\gamma, \\sigma_a, b, c) Now, in Bayesian inference, we usually regard (b, c, d, e) (b, c, d, e) as being fixed (and hence sacred), because they are the data that we have observed. By convention, in many probabilistic modelling problems, we\u2019re not really concerned about the data generating processes for (b, c, d, e) (b, c, d, e) because they aren\u2019t usually the sample-invariant, intrinsic parameter of a system we\u2019re interested in, unlike the Greek-letter symbols which take on their values independent of any given measured sample. Causal inference vs Bayesian inference, in brief I\u2019d now like to address \u201ccausal inference\u201d vs. \u201cbayesian inference\u201d; I think the use of a contrast is going to be helpful here. Bayesian inference asks the question, \u201cgiven the observed data and a presumed model with parameters, what is the expectation and uncertainty in the parameters that could have generated the observed data?\u201d Causal inference asks the question, \u201cgiven the observed data, what are plausible structures of the model that could have generated the observed data?\u201d Structures: in graphics and in equations By structures, we\u2019re really asking about the relationships between variables. Using the above example again, if a a is caused by b b and c c , then in abstract, we'd write that a = f(b, c) a = f(b, c) . We would then draw the following diagram to express the relationship: % config InlineBackend . figure_format = 'retina' import daft scale = 2.0 pgm = daft . PGM () pgm . add_node ( \"a\" , r \"$a$\" , 1.5 , 1 ) pgm . add_node ( \"b\" , r \"$b$\" , 1 , 2 ) pgm . add_node ( \"c\" , r \"$c$\" , 2 , 2 ) a_color = { \"ec\" : \"blue\" } pgm . add_edge ( \"b\" , \"a\" , plot_params = a_color ) pgm . add_edge ( \"c\" , \"a\" , plot_params = a_color ) pgm . render (); Likewise, since we presume that b b is caused by d d and e e , then the functional form of the causal relationship will be b = g(d, e) b = g(d, e) . We would then draw the following diagram to express the relationship: pgm = daft . PGM () pgm . add_node ( \"b\" , r \"$b$\" , 1.5 , 1 ) pgm . add_node ( \"d\" , r \"$d$\" , 1 , 2 ) pgm . add_node ( \"e\" , r \"$e$\" , 2 , 2 ) b_color = { \"ec\" : \"red\" } pgm . add_edge ( \"d\" , \"b\" , plot_params = b_color ) pgm . add_edge ( \"e\" , \"b\" , plot_params = b_color ) pgm . render (); And taken together, the full model would look like: pgm = daft . PGM () pgm . add_node ( \"a\" , r \"$a$\" , 1.5 , 1 ) pgm . add_node ( \"b\" , r \"$b$\" , 2 , 2 ) pgm . add_node ( \"c\" , r \"$c$\" , 1 , 2 ) pgm . add_node ( \"d\" , r \"$d$\" , 1.5 , 3 ) pgm . add_node ( \"e\" , r \"$e$\" , 2.5 , 3 ) pgm . add_edge ( \"c\" , \"a\" , plot_params = a_color ) pgm . add_edge ( \"b\" , \"a\" , plot_params = a_color ) pgm . add_edge ( \"d\" , \"b\" , plot_params = b_color ) pgm . add_edge ( \"e\" , \"b\" , plot_params = b_color ) pgm . add_text ( 3 , 1.25 , r \"$a = f(b, c) = \\beta b + \\gamma c + \\sigma_a$\" ) pgm . add_text ( 3 , 2.4 , r \"$b = f(d, e) = \\delta d + \\epsilon e + \\sigma_b$\" ) pgm . render (); In economics, the term \u201cstructural equation models\u201d refers to the system of equations that form the econometric model that economists build. Usually, those models are linear in nature. On occasion, though, additional functional forms might be used (sigmoidal, piecewise linear, neural network, etc.), if they help model the phenomena at hand. Whatever the form of the equation gets encapsulated into f(v_1, v_2, ..., v_n) f(v_1, v_2, ..., v_n) , where v_1... v_n v_1... v_n refer to the variables like b b and c c above. Structure, then, can refer to both the graphical structure of the model and the particular form of equations. As far as I have seen, most causal models assume some kind of linear equation between variables, though there are exceptions; in addition, causal inference, as taught, is usually concerned with inferring the graphical relationship between variables, presuming some linear form underneath. Inferring the model structure is what we call \"model inference\". A note for deep learners: this is what model inference actually is: inferring the structure of a model. More generally, if you think about linguistic convention, \u201cX inference\u201d usually refers to the tools and processes used in inferring X. Calling model inference the forward pass through the model breaks linguistic convention, and hence introduces viscosity in communication with others who adopt said linguistic convention! Restating the distinction At this point, I think it's an appropriate moment to try to re-state clearly what the relationship between a \"causal inference\" and a \"Bayesian inference\" is. They are both concerned with the system of equations that make up our model. However, in causal inference, we are primarily concerned with the relationship between observed variables, expressed as math equations. In Bayesian inference, we are primarily concerned with the parameters of those equations and their uncertainty. The do-operator Finally, we reach the point where we can touch the do-operator! This is such a key and crucial idea to causal methods, as it allows us to do counterfactual arguments conditioned on a presumed model. To illustrate what we mean by the do-operator, I'm going to rely on code, prose, and equations together. To start, let's implement the full probabilistic model above in Python code. import numpy as np from scipy.stats import norm import pandas as pd # Equations beta = 10 gamma = 0.3 delta = - 30 epsilon = 0.1 variance = 10 sigma_a_dist = norm ( 0 , variance ) sigma_b_dist = norm ( 0 , variance ) def a_equation ( b , c ): return beta * b + gamma * c + sigma_a_dist . rvs ( len ( b )) def b_equation ( d , e ): return delta * d + epsilon * e + sigma_b_dist . rvs ( len ( d )) To generate the data, we always begin with the nodes that have no parental nodes. In more complicated networks, we would leverage tools from network science, in this case, the topological sort , to identify the exact order in which we need to simulate observations. The observed data that we would end up collecting for this system looks like the following: N = 1_000 cc = norm ( 50 , 5 ) . rvs ( N ) dd = norm ( - 4 , 1 ) . rvs ( N ) ee = norm ( 30 , 4 ) . rvs ( N ) bb = b_equation ( dd , ee ) aa = a_equation ( bb , cc ) data = pd . DataFrame ({ \"a\" : aa , \"b\" : bb , \"c\" : cc , \"d\" : dd , \"e\" : ee }) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d e 0 1332.524476 131.382376 45.992774 -4.197128 31.860586 1 1373.132363 135.853147 43.887472 -4.164023 32.290897 2 1033.826746 101.218672 55.824587 -3.365145 25.216640 3 1480.752450 145.540974 49.372440 -4.589399 30.235521 4 1225.837710 120.624314 55.124833 -3.947604 28.537981 ... ... ... ... ... ... 995 1122.058675 112.318435 60.624883 -3.741055 23.348008 996 1271.828168 125.174992 38.599911 -3.998114 34.322930 997 936.356486 92.982304 45.385036 -3.185750 33.214713 998 1374.611126 134.746760 52.968263 -4.105797 17.916180 999 586.475070 58.313092 59.919246 -2.475159 27.182244 1000 rows \u00d7 5 columns And if we were to visualize the marginal distribution of each of the variables: import matplotlib.pyplot as plt fig , axes = plt . subplots ( figsize = ( 15 , 3 ), ncols = 5 ) for var , ax in zip ([ aa , bb , cc , dd , ee ], axes . flatten ()): ax . hist ( var ) plt . tight_layout () Finally, plotting the joint distributions, we get: import seaborn as sns sns . pairplot ( data ) <seaborn.axisgrid.PairGrid at 0x7f14016cc340> Now that we have a feel for what kind of data we'll collect, let's explore the three ways that the do-operator can be used. The do-operator: local counterfactuals At an elementary level, the do-operator expresses mathematically the following quesiton, \u201cFor a given sample, what if one of its variables took on a different value?\u201d You can think of this as a \"personalized\" counterfactual for a given sample, whether that sample is a patient, student, employee, citizen, or some other thing. Some concrete examples of this in action are: \u201cFor this given patient, what if the treatment given was the alternative treatment?\u201d \u201cFor this given student, what if they came from a wealthier household?\u201d \u201cFor this given tennis player, what if the court they played on were grass instead of clay?\u201d As you can see, one of the questions we\u2019re asking when asking counterfactuals are in fact personalized for a given sample. If we go back to the structural equations above, we could isolate a given observational data point (a, b, c) (a, b, c) and then ask the question, \u201cWhat if in row i i , b b took on a certain value B B instead of b_i b_i ?\u201d This question, when asked, results in our probabilistic structure changing a bit: We are now asking for P(a_i | do(b_i=B), \\beta, \\gamma, c_i) P(a_i | do(b_i=B), \\beta, \\gamma, c_i) , where i i refers to the particular sample index. Our counterfactual question presumes a known value of b b , and hence no longer requires us to generate it from (d, e) (d, e) . We can effectively cut (d, e) (d, e) out of the picture. As long as we preserve uncertainty in the parameter values, we can obtain counterfactual uncertainty as well. To illustrate how we do personalized counterfactuals in a Bayesian setting, let's see how to do it with PyMC. Implementation with PyMC We're going to start first by implementing the models. There's no do-operations happening just yet, we're just writing down the equations in PyMC first. import pymc as pm We first implement the set of equations. with pm . Model () as model_a : a = pm . Data ( \"a\" , data [ \"a\" ]) b = pm . Data ( \"b\" , data [ \"b\" ]) c = pm . Data ( \"c\" , data [ \"c\" ]) # Priors for model parameters beta = pm . Normal ( \"beta\" , mu = 10 , sigma = 3 ) gamma = pm . Normal ( \"gamma\" , mu = 0 , sigma = 10 ) sigma_a = pm . Exponential ( \"sigma_a\" , lam = 1 ) pm . Normal ( \"like\" , mu = beta * b + gamma * c , sigma = sigma_a , observed = a ) with pm . Model () as model_b : b = pm . Data ( \"b\" , data [ \"b\" ]) d = pm . Data ( \"d\" , data [ \"d\" ]) e = pm . Data ( \"e\" , data [ \"e\" ]) # Priors for model parameters delta = pm . Normal ( \"delta\" , mu =- 15 , sigma = 15 ) epsilon = pm . Normal ( \"epsilon\" , mu = 0 , sigma = 1 ) sigma_b = pm . Exponential ( \"sigma_b\" , lam = 1 ) pm . Normal ( \"like\" , mu = delta * d + epsilon * e , sigma = sigma_b , observed = b ) with pm . Model () as model_joint : a = pm . Data ( \"a\" , data [ \"a\" ]) b = pm . Data ( \"b\" , data [ \"b\" ]) c = pm . Data ( \"c\" , data [ \"c\" ]) d = pm . Data ( \"d\" , data [ \"d\" ]) e = pm . Data ( \"e\" , data [ \"e\" ]) # Priors for model parameters delta = pm . Normal ( \"delta\" , mu =- 15 , sigma = 15 ) epsilon = pm . Normal ( \"epsilon\" , mu = 0 , sigma = 1 ) sigma_b = pm . Exponential ( \"sigma_b\" , lam = 1 ) pm . Normal ( \"like_b\" , mu = delta * d + epsilon * e , sigma = sigma_b , observed = b ) # Priors for model parameters beta = pm . Normal ( \"beta\" , mu = 10 , sigma = 3 ) gamma = pm . Normal ( \"gamma\" , mu = 0 , sigma = 10 ) sigma_a = pm . Exponential ( \"sigma_a\" , lam = 1 ) pm . Normal ( \"like_a\" , mu = beta * b + gamma * c , sigma = sigma_a , observed = a ) /usr/share/miniconda3/envs/causality/lib/python3.9/site-packages/pymc/data.py:667: FutureWarning: The `mutable` kwarg was not specified. Currently it defaults to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default will change to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Set `pm.Data(..., mutable=False/True)`, or use `pm.ConstantData`/`pm.MutableData`. warnings.warn( /usr/share/miniconda3/envs/causality/lib/python3.9/site-packages/pymc/data.py:667: FutureWarning: The `mutable` kwarg was not specified. Currently it defaults to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default will change to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Set `pm.Data(..., mutable=False/True)`, or use `pm.ConstantData`/`pm.MutableData`. warnings.warn( Let's now plot the graphical model provided by PyMC. pm . model_to_graphviz ( model_a ) cluster1000 1000 c c ~ MutableData like like ~ Normal c->like a a ~ MutableData like->a b b ~ MutableData b->like beta beta ~ Normal beta->like sigma_a sigma_a ~ Exponential sigma_a->like gamma gamma ~ Normal gamma->like In this model, we see that a a , b b , and c c are all observed data, nested within the plate representing 1,000 data points. \\beta \\beta , \\gamma \\gamma , and \\sigma_a \\sigma_a are the parameters of the model that are invariant to any particular data point and hence are located outside of the plate. Those are our system-level parameters. An analogous diagram exists for b b 's model as well: pm . model_to_graphviz ( model_b ) cluster1000 1000 d d ~ MutableData like like ~ Normal d->like b b ~ MutableData like->b e e ~ MutableData e->like epsilon epsilon ~ Normal epsilon->like sigma_b sigma_b ~ Exponential sigma_b->like delta delta ~ Normal delta->like pm . model_to_graphviz ( model_joint ) cluster1000 1000 like_b like_b ~ Normal b b ~ MutableData like_b->b like_a like_a ~ Normal b->like_a e e ~ MutableData e->like_b d d ~ MutableData d->like_b a a ~ MutableData c c ~ MutableData c->like_a like_a->a epsilon epsilon ~ Normal epsilon->like_b sigma_a sigma_a ~ Exponential sigma_a->like_a beta beta ~ Normal beta->like_a delta delta ~ Normal delta->like_b gamma gamma ~ Normal gamma->like_a sigma_b sigma_b ~ Exponential sigma_b->like_b Parameter inference We can now use PyMC's inference machinery, the Inference Buttom (tm), to infer the values of the parameters above. with model_a : idata_a = pm . sample () Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Sequential sampling (2 chains in 1 job) NUTS: [beta, gamma, sigma_a] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:05<00:00 Sampling chain 0, 0 divergences] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:04<00:00 Sampling chain 1, 0 divergences] Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 10 seconds. with model_b : idata_b = pm . sample () Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Sequential sampling (2 chains in 1 job) NUTS: [delta, epsilon, sigma_b] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:06<00:00 Sampling chain 0, 0 divergences] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:04<00:00 Sampling chain 1, 0 divergences] Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 11 seconds. The acceptance probability does not match the target. It is 0.8982, but should be close to 0.8. Try to increase the number of tuning steps. with model_joint : idata_joint = pm . sample () Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Sequential sampling (2 chains in 1 job) NUTS: [delta, epsilon, sigma_b, beta, gamma, sigma_a] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:17<00:00 Sampling chain 0, 0 divergences] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:11<00:00 Sampling chain 1, 0 divergences] Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 29 seconds. The acceptance probability does not match the target. It is 0.9213, but should be close to 0.8. Try to increase the number of tuning steps. I will be skipping over a rigorous Bayesian workflow here because it is not the point of the notebook. Nonetheless, here is a set of plots for our posteriors; you can check-them against the original Gaussian distributions above. import arviz as az az . plot_posterior ( idata_a , ref_val = [ 10 , 0.3 , 10 ]) array([<AxesSubplot:title={'center':'beta'}>, <AxesSubplot:title={'center':'gamma'}>, <AxesSubplot:title={'center':'sigma_a'}>], dtype=object) az . plot_posterior ( idata_b , ref_val = [ - 30 , 0.1 , 10 ]) array([<AxesSubplot:title={'center':'delta'}>, <AxesSubplot:title={'center':'epsilon'}>, <AxesSubplot:title={'center':'sigma_b'}>], dtype=object) az . plot_posterior ( idata_joint ) array([[<AxesSubplot:title={'center':'delta'}>, <AxesSubplot:title={'center':'epsilon'}>, <AxesSubplot:title={'center':'beta'}>], [<AxesSubplot:title={'center':'gamma'}>, <AxesSubplot:title={'center':'sigma_b'}>, <AxesSubplot:title={'center':'sigma_a'}>]], dtype=object) In all of the cases, we're pretty close, though a bit off. As the goal here is to show a personalized counterfactual, we're not going to worry too much about the parameter recovery accuracy. Personalized counterfactuals in PyMC Personalized counterfactuals ask the question, \"What would have happened for a particular sample, i.e. one row in the dataset, had one of its observed variables been a different value?\" In this case, we're going to take one of the observations and ask what would happen if we counterfactually set c c to a different value. We'll start by isolating a sample of interest: sample = data . sample ( random_state = 491 ) # random state for reproducibility sample .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d e 245 1578.857375 157.056435 56.295492 -4.937509 25.404555 We then simulate what the posterior for that sample would look like under the original case vs. the counterfactual case. In the first code block below, we set our data variables to be a single sample and then leverage the posterior predictive samples to simulate what a would look like under the original data. In the second code block, we set our data variables to be a single sample as well, except that we have changed c to be a totally different value from what it was before. This gives us our counterfactual scenario! import numpy as np with model_a : # Simulate posterior predictive under original observed data. pm . set_data ({ \"a\" : sample [ \"a\" ], \"b\" : sample [ \"b\" ], \"c\" : sample [ \"c\" ]}) ppc_or = pm . sample_posterior_predictive ( idata_a ) # Simulate posterior predictive under a counterfactual scenario. # ****This is the do-operator in action!**** pm . set_data ({ \"a\" : sample [ \"a\" ], \"b\" : sample [ \"b\" ], \"c\" : sample [ \"c\" ] / 10 }) ppc_cf = pm . sample_posterior_predictive ( idata_a ) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:00<00:00] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:00<00:00] ppc_or . posterior_predictive /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } html[theme=dark], body[data-theme=dark], body.vscode-dark { --xr-font-color0: rgba(255, 255, 255, 1); --xr-font-color2: rgba(255, 255, 255, 0.54); --xr-font-color3: rgba(255, 255, 255, 0.38); --xr-border-color: #1F1F1F; --xr-disabled-color: #515151; --xr-background-color: #111111; --xr-background-color-row-even: #111111; --xr-background-color-row-odd: #313131; } .xr-wrap { display: block !important; min-width: 300px; max-width: 700px; } .xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } <xarray.Dataset> Dimensions: (chain: 2, draw: 1000, like_dim_0: 1) Coordinates: * chain (chain) int64 0 1 * draw (draw) int64 0 1 2 3 4 5 6 7 ... 992 993 994 995 996 997 998 999 * like_dim_0 (like_dim_0) int64 0 Data variables: like (chain, draw, like_dim_0) float64 1.587e+03 ... 1.584e+03 Attributes: created_at: 2022-07-23T14:10:04.397013 arviz_version: 0.12.1 inference_library: pymc inference_library_version: 4.0.1 xarray.Dataset Dimensions: chain : 2 draw : 1000 like_dim_0 : 1 Coordinates: (3) chain (chain) int64 0 1 array([0, 1]) draw (draw) int64 0 1 2 3 4 5 ... 995 996 997 998 999 array([ 0, 1, 2, ..., 997, 998, 999]) like_dim_0 (like_dim_0) int64 0 array([0]) Data variables: (1) like (chain, draw, like_dim_0) float64 1.587e+03 1.579e+03 ... 1.584e+03 array([[[1587.07786376], [1579.22142173], [1591.73356013], ..., [1601.59229977], [1578.46236853], [1591.30863738]], [[1590.83753783], [1573.04613255], [1574.44687758], ..., [1575.81574783], [1576.57422809], [1584.35188303]]]) Attributes: (4) created_at : 2022-07-23T14:10:04.397013 arviz_version : 0.12.1 inference_library : pymc inference_library_version : 4.0.1 fig , axes = plt . subplots ( figsize = ( 10 , 5 ), ncols = 2 ) az . plot_posterior ( ppc_or . posterior_predictive [ \"like\" ], ax = axes [ 0 ]) axes [ 0 ] . set_title ( \"Original\" ) az . plot_posterior ( ppc_cf . posterior_predictive [ \"like\" ], ax = axes [ 1 ]) axes [ 1 ] . set_title ( \"Counterfactual\" ) Text(0.5, 1.0, 'Counterfactual') What's really cool here is that we've done a personalized counterfactual for that particular student. In this case, we set the cost of the student's tuition fees to 1/10 the original, akin to sending them from a private school to a public school. Under this particularly flawed model, we would expect their sum of grades to go down, though not by a large magnitude. The key trick to enabling this was using pm.Data() containers for our data, thus registering them with PyMC (and Aesara underneath) as being hot-swappable entities with pm.set_data() . Finally, because our posterior distributions contain our probabilistic beliefs having seen the data, or more colloquially, \"fits of our parameters\", we sample from the posterior predictive distribution to identify what we would have gotten in a counterfactual situation. The do-operator: global counterfactuals Let\u2019s also think about another possible counterfactual question: What if the system parameters were different? Recall here that \u201csystem parameters\u201d refer to the linear coefficients. They aren\u2019t properties of any observation (or sample), but are properties of the entire system as a whole, hence the moniker \u201cglobal\u201d counterfactuals. To do global counterfactuals, we actually need to condition the coefficients\u2019 values on a particular value, just like we did for data on the personalized counterfactual. For example, if we conditioned \\beta \\beta to be equal to 0, in other words severing the relationship between a a and b b , then: We are now asking for P(a|do(\\beta=0), \\gamma, b, c) P(a|do(\\beta=0), \\gamma, b, c) . (Notice the omission of subscript i i , it\u2019s intentional!) Our counterfactual question presumes a known value of \\beta \\beta , but not a presumed known value of any of (b, c) (b, c) . (We aren't asking about any particular sample, after all!) One way we can implement this do-operation is to reconstruct the model from our posteriors while hard-coding the value of \\beta \\beta to 0. Let's see how to make this happen. Firstly, in order to take advantage of our fitted posteriors, we will use a from_posterior function written implemented in the PyMC how-to guides. from scipy import stats # Credit: https://docs.pymc.io/projects/examples/en/latest/pymc3_howto/updating_priors.html def from_posterior ( param , samples ): smin , smax = np . min ( samples ), np . max ( samples ) width = smax - smin x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np . concatenate ([[ x [ 0 ] - 1 * width ], x , [ x [ - 1 ] + 1 * width ]]) y = np . concatenate ([[ 0 ], y , [ 0 ]]) return pm . Interpolated ( param , x , y ) Then, we'll need to rewrite the model a little bit, this time hard-coding one of the variables to a particular value. with pm . Model () as model_a_modified : a = pm . Data ( \"a\" , data [ \"a\" ]) b = pm . Data ( \"b\" , data [ \"b\" ]) c = pm . Data ( \"c\" , data [ \"c\" ]) # Model parameters beta = 0 # ****This is the do-operator in action!**** gamma = from_posterior ( \"gamma\" , idata_a . posterior [ \"gamma\" ] . values . flatten ()) sigma_a = from_posterior ( \"sigma_a\" , idata_a . posterior [ \"sigma_a\" ] . values . flatten ()) pm . Normal ( \"like\" , mu = beta * b + gamma * c , sigma = sigma_a , observed = a ) /usr/share/miniconda3/envs/causality/lib/python3.9/site-packages/pymc/data.py:667: FutureWarning: The `mutable` kwarg was not specified. Currently it defaults to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default will change to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Set `pm.Data(..., mutable=False/True)`, or use `pm.ConstantData`/`pm.MutableData`. warnings.warn( Finally, since our posteriors have become priors in this new model, we sample from the prior predictive distribution: with model_a_modified : trace = pm . sample_prior_predictive () /usr/share/miniconda3/envs/causality/lib/python3.9/site-packages/pymc/distributions/continuous.py:3704: RuntimeWarning: divide by zero encountered in double_scalars np.where(np.abs(pdf[index]) <= 1e-8, np.zeros(index.shape), (p - cdf[index]) / pdf[index]), Now, we're able to examine how our observations have changed based on the intervention at the system. def plot_global_counterfactual ( idx : int ): az . plot_posterior ( trace . prior_predictive [ \"like\" ][ 0 , :, idx ]) plt . gca () . set_title ( f \"Original value: { data . loc [ idx , 'a' ] : .2f } \" ) plot_global_counterfactual ( 2 ) plot_global_counterfactual ( 3 ) We can see here that by severing the connection between the IQ of a student and their grades, each student's total sum of grades goes down to 1/20th of their original. Those of us smarter than this author would probably have intuited this point without needing to code it up (by examining the magnitude of the slope coefficients), but in the case of exotic functional forms with interaction terms (or a neural net structure), the value of an explicit, global perturbation of an interpretable parameter is quite evident! The do-operator: combining global and personalized counterfactuals If we\u2019re thinking logically here, we\u2019ll soon realize that it\u2019s also possible to combine the two aforementioned counterfactuals together. We can ask the question, what is P(a_i | do(\\beta=0, b_i=B), \\gamma, c_i) P(a_i | do(\\beta=0, b_i=B), \\gamma, c_i) ? (Note again the presence of the index i i !) Algorithmically, this question essentially translates to: picking out sample i i , setting b=B b=B , setting beta=3.14 beta=3.14 , and evaluating what a_i a_i would look like under those two conditions. For the sake of illustration, here it is in PyMC code: with model_a_modified : pm . set_data ({ \"a\" : sample [ \"a\" ], \"b\" : sample [ \"b\" ], \"c\" : sample [ \"c\" ]}) ppc_or = pm . sample_prior_predictive () # cf = \"counterfactual\" pm . set_data ({ \"a\" : sample [ \"a\" ], \"b\" : sample [ \"b\" ], \"c\" : sample [ \"c\" ] * 0.1 }) ppc_cf = pm . sample_prior_predictive () fig , axes = plt . subplots ( figsize = ( 15 , 5 ), ncols = 2 ) az . plot_posterior ( ppc_or . prior_predictive [ \"like\" ], ax = axes [ 0 ]) axes [ 0 ] . set_title ( r \"Distribution of $a$ with $\\beta=0$ and $c = c_i$\" ) az . plot_posterior ( ppc_cf . prior_predictive [ \"like\" ], ax = axes [ 1 ]) axes [ 1 ] . set_title ( r \"Distribution of $a$ with $\\beta=0$ and $c = \\frac {c_i}{10} $\" ) Text(0.5, 1.0, 'Distribution of $a$ with $\\\\beta=0$ and $c = \\\\frac{c_i}{10}$') Framework Having worked through this example, we've finally arrived at a framework for thinking through the connection between Bayesian models and causal models. To know which kind of counterfactual we need to employ, we have to be extremely clear on the exact question we\u2019re trying to ask. Are we trying to ask: A personalized question? (\u201dWhat would have happened to this particular sample had its dependent variable been set to a particular value?\u201d) A systems-level question? (\u201dWhat would happen to all of our observations if a system parameter was set to a particular value?\u201d) A hybrid question? (\u201dWhat would have happened to this particular sample had its dependent variable been set to a particular value and the system parameter set to a particular value?\u201d) If we have our variables\u2019 dependencies clearly and explicitly stated, then it becomes easy to ask the do-operator family of questions, which basically are asking, \u201cWhat happens if we set something in the model to a particular value?\u201d Summary In this notebook, we have seen how to do causal inference on Bayesian models written in PyMC. We started with the contrast and comparison between causal and Bayesian inference. Then, we went through the logical framework of asking counterfactuals on (1) a per-sample basis, (2) a system-wide basis, and (3) both together. The value of the do-operator, and by extension causal methods, is that they provide us the ability to ask these \"what if\" questions in cases where conducting an experiment would be unethical, cost-prohibitive, or logistically challenging. Combining it with Bayesian models gives us the ability to assess what would have happened in a counterfactual world (causal) while also calculating the full range of possibilities (Bayesian). Blending the two together is nothing more than a matter of logic! Credits I would like to credit Thomas Wiecki and Ben Vincent for reviewing an early draft of this notebook.","title":"07: The Do Operator"},{"location":"07-do-operator/#the-do-operator-and-bayesian-models","text":"The do-operator is a foundational idea in causal methods, used to express, in precise mathematical language, what counterfactual interventions would look like. The do-operator from causal inference has a tight connection to probabilistic modelling. But what exactly is that connection? Having myself been previously confused about the link between graphical models, causal structure, and more, here's my current understanding of the connection. What I hope you'll see here is that the do-operator gives us the ability to simulate what an experiment might look like... without actually doing the experiment, and that at its implementation core, it ends up being nothing more than \"set this thing to a particular value\". If you're curious now, it's time to read on :).","title":"The Do-Operator and Bayesian Models"},{"location":"07-do-operator/#causal-models-and-bayesian-models-in-brief","text":"Let\u2019s start first with a brief overview of the connection between causal models and Bayesian models. In doing my own study on causality , it soon became clear to me that causal models can be read off from mathematical equations quite easily. For a moment, let's assume we have the following set of equations that we presume to describe some data that we observe: b = \\delta d + \\epsilon e + \\sigma_b b = \\delta d + \\epsilon e + \\sigma_b and a = \\beta b + \\gamma c + \\sigma_a a = \\beta b + \\gamma c + \\sigma_a My convention here is that the English letters represent data, while the Greek letters represent parameters of the model. To make this abstract example a bit more concrete, let's connect these equations to something in real life, such as scholastic achievement. In this highly contrived and fictitious example, let's say that a a is the scholastic achievement of a student as measured by the sum total of subject scores across 11 subjects in the O-level examinations administered by the University of Cambridge, and it is thought to be a function of: b b , the intelligence quotient (IQ) of the student, c c , the cost of a student's tuition fees in thousands of dollars (private vs. public school), and \\sigma_a \\sigma_a , the intrinsic variation in student performance as a whole For the term b b , we think it is a function of: d d , the crime rate of their neighborhood, e e , the household income of that student, in thousands of dollars, and \\sigma_b \\sigma_b , the intrinsic variation in IQ. The astute reader will immediately see the controversy in this example. The causal path we assume here isn't going to be the only plausible model, and there are many, many variables we're leaving out here. However, I'd ask that you let your mind suspend the controversy for a moment; it's always a big brain teaser to try to come up with plausible but fictitious examples. To read off the causal model from this set of equations, everything on the right is causal for the stuff on the left. By those equations, we state that the values of a a that we observe are caused by values of b b and c c and their Greek coefficients \\beta \\beta and \\gamma \\gamma plus some noise \\sigma_a \\sigma_a , while the values of b b that we observe are caused by the values of d d and e e and their Greek coefficients \\delta \\delta and \\epsilon \\epsilon plus some noise \\sigma_b \\sigma_b . With Bayesian inference on this model, our task here is to estimate the coefficients , \\beta \\beta , \\gamma \\gamma , \\delta \\delta , and \\epsilon \\epsilon . More precisely, we are interested in estimating their expected value and uncertainty. In order to express the model in such a way that reflects its probabilistic in nature, we need some syntax to express the idea that because \\delta \\delta , \\epsilon \\epsilon and \\sigma_b \\sigma_b are uncertain and are modelled by a probability distribution, therefore b b is uncertain and has a corresponding probability distribution too. At the same time, we need an analogous syntax to express that because \\beta \\beta , \\gamma \\gamma , and \\sigma_a \\sigma_a are uncertain and are modelled by a probability distribution, therefore a a is also uncertain and has a corresponding probability distribution too. Moreover, if we assume that c c , d d , e e are stochastic because they are drawn from a distribution, then we have ~~a classic case where everything is unknown and we can\u2019t do anything~~ an awesome modelling problem at hand! \ud83d\ude42 In any case, for the first equation, our expression for the distribution of b b conditioned on everything on the right would look something like: P(b | \\delta, \\epsilon, \\sigma_b, d, e) P(b | \\delta, \\epsilon, \\sigma_b, d, e) And likewise, the distribution of a a conditioned on everything on the right would look something like this: P(a | \\beta, \\gamma, \\sigma_a, b, c) P(a | \\beta, \\gamma, \\sigma_a, b, c) Now, in Bayesian inference, we usually regard (b, c, d, e) (b, c, d, e) as being fixed (and hence sacred), because they are the data that we have observed. By convention, in many probabilistic modelling problems, we\u2019re not really concerned about the data generating processes for (b, c, d, e) (b, c, d, e) because they aren\u2019t usually the sample-invariant, intrinsic parameter of a system we\u2019re interested in, unlike the Greek-letter symbols which take on their values independent of any given measured sample.","title":"Causal models and Bayesian models, in brief"},{"location":"07-do-operator/#causal-inference-vs-bayesian-inference-in-brief","text":"I\u2019d now like to address \u201ccausal inference\u201d vs. \u201cbayesian inference\u201d; I think the use of a contrast is going to be helpful here. Bayesian inference asks the question, \u201cgiven the observed data and a presumed model with parameters, what is the expectation and uncertainty in the parameters that could have generated the observed data?\u201d Causal inference asks the question, \u201cgiven the observed data, what are plausible structures of the model that could have generated the observed data?\u201d","title":"Causal inference vs Bayesian inference, in brief"},{"location":"07-do-operator/#structures-in-graphics-and-in-equations","text":"By structures, we\u2019re really asking about the relationships between variables. Using the above example again, if a a is caused by b b and c c , then in abstract, we'd write that a = f(b, c) a = f(b, c) . We would then draw the following diagram to express the relationship: % config InlineBackend . figure_format = 'retina' import daft scale = 2.0 pgm = daft . PGM () pgm . add_node ( \"a\" , r \"$a$\" , 1.5 , 1 ) pgm . add_node ( \"b\" , r \"$b$\" , 1 , 2 ) pgm . add_node ( \"c\" , r \"$c$\" , 2 , 2 ) a_color = { \"ec\" : \"blue\" } pgm . add_edge ( \"b\" , \"a\" , plot_params = a_color ) pgm . add_edge ( \"c\" , \"a\" , plot_params = a_color ) pgm . render (); Likewise, since we presume that b b is caused by d d and e e , then the functional form of the causal relationship will be b = g(d, e) b = g(d, e) . We would then draw the following diagram to express the relationship: pgm = daft . PGM () pgm . add_node ( \"b\" , r \"$b$\" , 1.5 , 1 ) pgm . add_node ( \"d\" , r \"$d$\" , 1 , 2 ) pgm . add_node ( \"e\" , r \"$e$\" , 2 , 2 ) b_color = { \"ec\" : \"red\" } pgm . add_edge ( \"d\" , \"b\" , plot_params = b_color ) pgm . add_edge ( \"e\" , \"b\" , plot_params = b_color ) pgm . render (); And taken together, the full model would look like: pgm = daft . PGM () pgm . add_node ( \"a\" , r \"$a$\" , 1.5 , 1 ) pgm . add_node ( \"b\" , r \"$b$\" , 2 , 2 ) pgm . add_node ( \"c\" , r \"$c$\" , 1 , 2 ) pgm . add_node ( \"d\" , r \"$d$\" , 1.5 , 3 ) pgm . add_node ( \"e\" , r \"$e$\" , 2.5 , 3 ) pgm . add_edge ( \"c\" , \"a\" , plot_params = a_color ) pgm . add_edge ( \"b\" , \"a\" , plot_params = a_color ) pgm . add_edge ( \"d\" , \"b\" , plot_params = b_color ) pgm . add_edge ( \"e\" , \"b\" , plot_params = b_color ) pgm . add_text ( 3 , 1.25 , r \"$a = f(b, c) = \\beta b + \\gamma c + \\sigma_a$\" ) pgm . add_text ( 3 , 2.4 , r \"$b = f(d, e) = \\delta d + \\epsilon e + \\sigma_b$\" ) pgm . render (); In economics, the term \u201cstructural equation models\u201d refers to the system of equations that form the econometric model that economists build. Usually, those models are linear in nature. On occasion, though, additional functional forms might be used (sigmoidal, piecewise linear, neural network, etc.), if they help model the phenomena at hand. Whatever the form of the equation gets encapsulated into f(v_1, v_2, ..., v_n) f(v_1, v_2, ..., v_n) , where v_1... v_n v_1... v_n refer to the variables like b b and c c above. Structure, then, can refer to both the graphical structure of the model and the particular form of equations. As far as I have seen, most causal models assume some kind of linear equation between variables, though there are exceptions; in addition, causal inference, as taught, is usually concerned with inferring the graphical relationship between variables, presuming some linear form underneath. Inferring the model structure is what we call \"model inference\". A note for deep learners: this is what model inference actually is: inferring the structure of a model. More generally, if you think about linguistic convention, \u201cX inference\u201d usually refers to the tools and processes used in inferring X. Calling model inference the forward pass through the model breaks linguistic convention, and hence introduces viscosity in communication with others who adopt said linguistic convention!","title":"Structures: in graphics and in equations"},{"location":"07-do-operator/#restating-the-distinction","text":"At this point, I think it's an appropriate moment to try to re-state clearly what the relationship between a \"causal inference\" and a \"Bayesian inference\" is. They are both concerned with the system of equations that make up our model. However, in causal inference, we are primarily concerned with the relationship between observed variables, expressed as math equations. In Bayesian inference, we are primarily concerned with the parameters of those equations and their uncertainty.","title":"Restating the distinction"},{"location":"07-do-operator/#the-do-operator","text":"Finally, we reach the point where we can touch the do-operator! This is such a key and crucial idea to causal methods, as it allows us to do counterfactual arguments conditioned on a presumed model. To illustrate what we mean by the do-operator, I'm going to rely on code, prose, and equations together. To start, let's implement the full probabilistic model above in Python code. import numpy as np from scipy.stats import norm import pandas as pd # Equations beta = 10 gamma = 0.3 delta = - 30 epsilon = 0.1 variance = 10 sigma_a_dist = norm ( 0 , variance ) sigma_b_dist = norm ( 0 , variance ) def a_equation ( b , c ): return beta * b + gamma * c + sigma_a_dist . rvs ( len ( b )) def b_equation ( d , e ): return delta * d + epsilon * e + sigma_b_dist . rvs ( len ( d )) To generate the data, we always begin with the nodes that have no parental nodes. In more complicated networks, we would leverage tools from network science, in this case, the topological sort , to identify the exact order in which we need to simulate observations. The observed data that we would end up collecting for this system looks like the following: N = 1_000 cc = norm ( 50 , 5 ) . rvs ( N ) dd = norm ( - 4 , 1 ) . rvs ( N ) ee = norm ( 30 , 4 ) . rvs ( N ) bb = b_equation ( dd , ee ) aa = a_equation ( bb , cc ) data = pd . DataFrame ({ \"a\" : aa , \"b\" : bb , \"c\" : cc , \"d\" : dd , \"e\" : ee }) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d e 0 1332.524476 131.382376 45.992774 -4.197128 31.860586 1 1373.132363 135.853147 43.887472 -4.164023 32.290897 2 1033.826746 101.218672 55.824587 -3.365145 25.216640 3 1480.752450 145.540974 49.372440 -4.589399 30.235521 4 1225.837710 120.624314 55.124833 -3.947604 28.537981 ... ... ... ... ... ... 995 1122.058675 112.318435 60.624883 -3.741055 23.348008 996 1271.828168 125.174992 38.599911 -3.998114 34.322930 997 936.356486 92.982304 45.385036 -3.185750 33.214713 998 1374.611126 134.746760 52.968263 -4.105797 17.916180 999 586.475070 58.313092 59.919246 -2.475159 27.182244 1000 rows \u00d7 5 columns And if we were to visualize the marginal distribution of each of the variables: import matplotlib.pyplot as plt fig , axes = plt . subplots ( figsize = ( 15 , 3 ), ncols = 5 ) for var , ax in zip ([ aa , bb , cc , dd , ee ], axes . flatten ()): ax . hist ( var ) plt . tight_layout () Finally, plotting the joint distributions, we get: import seaborn as sns sns . pairplot ( data ) <seaborn.axisgrid.PairGrid at 0x7f14016cc340> Now that we have a feel for what kind of data we'll collect, let's explore the three ways that the do-operator can be used.","title":"The do-operator"},{"location":"07-do-operator/#the-do-operator-local-counterfactuals","text":"At an elementary level, the do-operator expresses mathematically the following quesiton, \u201cFor a given sample, what if one of its variables took on a different value?\u201d You can think of this as a \"personalized\" counterfactual for a given sample, whether that sample is a patient, student, employee, citizen, or some other thing. Some concrete examples of this in action are: \u201cFor this given patient, what if the treatment given was the alternative treatment?\u201d \u201cFor this given student, what if they came from a wealthier household?\u201d \u201cFor this given tennis player, what if the court they played on were grass instead of clay?\u201d As you can see, one of the questions we\u2019re asking when asking counterfactuals are in fact personalized for a given sample. If we go back to the structural equations above, we could isolate a given observational data point (a, b, c) (a, b, c) and then ask the question, \u201cWhat if in row i i , b b took on a certain value B B instead of b_i b_i ?\u201d This question, when asked, results in our probabilistic structure changing a bit: We are now asking for P(a_i | do(b_i=B), \\beta, \\gamma, c_i) P(a_i | do(b_i=B), \\beta, \\gamma, c_i) , where i i refers to the particular sample index. Our counterfactual question presumes a known value of b b , and hence no longer requires us to generate it from (d, e) (d, e) . We can effectively cut (d, e) (d, e) out of the picture. As long as we preserve uncertainty in the parameter values, we can obtain counterfactual uncertainty as well. To illustrate how we do personalized counterfactuals in a Bayesian setting, let's see how to do it with PyMC.","title":"The do-operator: local counterfactuals"},{"location":"07-do-operator/#implementation-with-pymc","text":"We're going to start first by implementing the models. There's no do-operations happening just yet, we're just writing down the equations in PyMC first. import pymc as pm We first implement the set of equations. with pm . Model () as model_a : a = pm . Data ( \"a\" , data [ \"a\" ]) b = pm . Data ( \"b\" , data [ \"b\" ]) c = pm . Data ( \"c\" , data [ \"c\" ]) # Priors for model parameters beta = pm . Normal ( \"beta\" , mu = 10 , sigma = 3 ) gamma = pm . Normal ( \"gamma\" , mu = 0 , sigma = 10 ) sigma_a = pm . Exponential ( \"sigma_a\" , lam = 1 ) pm . Normal ( \"like\" , mu = beta * b + gamma * c , sigma = sigma_a , observed = a ) with pm . Model () as model_b : b = pm . Data ( \"b\" , data [ \"b\" ]) d = pm . Data ( \"d\" , data [ \"d\" ]) e = pm . Data ( \"e\" , data [ \"e\" ]) # Priors for model parameters delta = pm . Normal ( \"delta\" , mu =- 15 , sigma = 15 ) epsilon = pm . Normal ( \"epsilon\" , mu = 0 , sigma = 1 ) sigma_b = pm . Exponential ( \"sigma_b\" , lam = 1 ) pm . Normal ( \"like\" , mu = delta * d + epsilon * e , sigma = sigma_b , observed = b ) with pm . Model () as model_joint : a = pm . Data ( \"a\" , data [ \"a\" ]) b = pm . Data ( \"b\" , data [ \"b\" ]) c = pm . Data ( \"c\" , data [ \"c\" ]) d = pm . Data ( \"d\" , data [ \"d\" ]) e = pm . Data ( \"e\" , data [ \"e\" ]) # Priors for model parameters delta = pm . Normal ( \"delta\" , mu =- 15 , sigma = 15 ) epsilon = pm . Normal ( \"epsilon\" , mu = 0 , sigma = 1 ) sigma_b = pm . Exponential ( \"sigma_b\" , lam = 1 ) pm . Normal ( \"like_b\" , mu = delta * d + epsilon * e , sigma = sigma_b , observed = b ) # Priors for model parameters beta = pm . Normal ( \"beta\" , mu = 10 , sigma = 3 ) gamma = pm . Normal ( \"gamma\" , mu = 0 , sigma = 10 ) sigma_a = pm . Exponential ( \"sigma_a\" , lam = 1 ) pm . Normal ( \"like_a\" , mu = beta * b + gamma * c , sigma = sigma_a , observed = a ) /usr/share/miniconda3/envs/causality/lib/python3.9/site-packages/pymc/data.py:667: FutureWarning: The `mutable` kwarg was not specified. Currently it defaults to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default will change to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Set `pm.Data(..., mutable=False/True)`, or use `pm.ConstantData`/`pm.MutableData`. warnings.warn( /usr/share/miniconda3/envs/causality/lib/python3.9/site-packages/pymc/data.py:667: FutureWarning: The `mutable` kwarg was not specified. Currently it defaults to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default will change to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Set `pm.Data(..., mutable=False/True)`, or use `pm.ConstantData`/`pm.MutableData`. warnings.warn( Let's now plot the graphical model provided by PyMC. pm . model_to_graphviz ( model_a ) cluster1000 1000 c c ~ MutableData like like ~ Normal c->like a a ~ MutableData like->a b b ~ MutableData b->like beta beta ~ Normal beta->like sigma_a sigma_a ~ Exponential sigma_a->like gamma gamma ~ Normal gamma->like In this model, we see that a a , b b , and c c are all observed data, nested within the plate representing 1,000 data points. \\beta \\beta , \\gamma \\gamma , and \\sigma_a \\sigma_a are the parameters of the model that are invariant to any particular data point and hence are located outside of the plate. Those are our system-level parameters. An analogous diagram exists for b b 's model as well: pm . model_to_graphviz ( model_b ) cluster1000 1000 d d ~ MutableData like like ~ Normal d->like b b ~ MutableData like->b e e ~ MutableData e->like epsilon epsilon ~ Normal epsilon->like sigma_b sigma_b ~ Exponential sigma_b->like delta delta ~ Normal delta->like pm . model_to_graphviz ( model_joint ) cluster1000 1000 like_b like_b ~ Normal b b ~ MutableData like_b->b like_a like_a ~ Normal b->like_a e e ~ MutableData e->like_b d d ~ MutableData d->like_b a a ~ MutableData c c ~ MutableData c->like_a like_a->a epsilon epsilon ~ Normal epsilon->like_b sigma_a sigma_a ~ Exponential sigma_a->like_a beta beta ~ Normal beta->like_a delta delta ~ Normal delta->like_b gamma gamma ~ Normal gamma->like_a sigma_b sigma_b ~ Exponential sigma_b->like_b","title":"Implementation with PyMC"},{"location":"07-do-operator/#parameter-inference","text":"We can now use PyMC's inference machinery, the Inference Buttom (tm), to infer the values of the parameters above. with model_a : idata_a = pm . sample () Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Sequential sampling (2 chains in 1 job) NUTS: [beta, gamma, sigma_a] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:05<00:00 Sampling chain 0, 0 divergences] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:04<00:00 Sampling chain 1, 0 divergences] Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 10 seconds. with model_b : idata_b = pm . sample () Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Sequential sampling (2 chains in 1 job) NUTS: [delta, epsilon, sigma_b] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:06<00:00 Sampling chain 0, 0 divergences] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:04<00:00 Sampling chain 1, 0 divergences] Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 11 seconds. The acceptance probability does not match the target. It is 0.8982, but should be close to 0.8. Try to increase the number of tuning steps. with model_joint : idata_joint = pm . sample () Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Sequential sampling (2 chains in 1 job) NUTS: [delta, epsilon, sigma_b, beta, gamma, sigma_a] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:17<00:00 Sampling chain 0, 0 divergences] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:11<00:00 Sampling chain 1, 0 divergences] Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 29 seconds. The acceptance probability does not match the target. It is 0.9213, but should be close to 0.8. Try to increase the number of tuning steps. I will be skipping over a rigorous Bayesian workflow here because it is not the point of the notebook. Nonetheless, here is a set of plots for our posteriors; you can check-them against the original Gaussian distributions above. import arviz as az az . plot_posterior ( idata_a , ref_val = [ 10 , 0.3 , 10 ]) array([<AxesSubplot:title={'center':'beta'}>, <AxesSubplot:title={'center':'gamma'}>, <AxesSubplot:title={'center':'sigma_a'}>], dtype=object) az . plot_posterior ( idata_b , ref_val = [ - 30 , 0.1 , 10 ]) array([<AxesSubplot:title={'center':'delta'}>, <AxesSubplot:title={'center':'epsilon'}>, <AxesSubplot:title={'center':'sigma_b'}>], dtype=object) az . plot_posterior ( idata_joint ) array([[<AxesSubplot:title={'center':'delta'}>, <AxesSubplot:title={'center':'epsilon'}>, <AxesSubplot:title={'center':'beta'}>], [<AxesSubplot:title={'center':'gamma'}>, <AxesSubplot:title={'center':'sigma_b'}>, <AxesSubplot:title={'center':'sigma_a'}>]], dtype=object) In all of the cases, we're pretty close, though a bit off. As the goal here is to show a personalized counterfactual, we're not going to worry too much about the parameter recovery accuracy.","title":"Parameter inference"},{"location":"07-do-operator/#personalized-counterfactuals-in-pymc","text":"Personalized counterfactuals ask the question, \"What would have happened for a particular sample, i.e. one row in the dataset, had one of its observed variables been a different value?\" In this case, we're going to take one of the observations and ask what would happen if we counterfactually set c c to a different value. We'll start by isolating a sample of interest: sample = data . sample ( random_state = 491 ) # random state for reproducibility sample .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a b c d e 245 1578.857375 157.056435 56.295492 -4.937509 25.404555 We then simulate what the posterior for that sample would look like under the original case vs. the counterfactual case. In the first code block below, we set our data variables to be a single sample and then leverage the posterior predictive samples to simulate what a would look like under the original data. In the second code block, we set our data variables to be a single sample as well, except that we have changed c to be a totally different value from what it was before. This gives us our counterfactual scenario! import numpy as np with model_a : # Simulate posterior predictive under original observed data. pm . set_data ({ \"a\" : sample [ \"a\" ], \"b\" : sample [ \"b\" ], \"c\" : sample [ \"c\" ]}) ppc_or = pm . sample_posterior_predictive ( idata_a ) # Simulate posterior predictive under a counterfactual scenario. # ****This is the do-operator in action!**** pm . set_data ({ \"a\" : sample [ \"a\" ], \"b\" : sample [ \"b\" ], \"c\" : sample [ \"c\" ] / 10 }) ppc_cf = pm . sample_posterior_predictive ( idata_a ) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:00<00:00] /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } 100.00% [2000/2000 00:00<00:00] ppc_or . posterior_predictive /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } html[theme=dark], body[data-theme=dark], body.vscode-dark { --xr-font-color0: rgba(255, 255, 255, 1); --xr-font-color2: rgba(255, 255, 255, 0.54); --xr-font-color3: rgba(255, 255, 255, 0.38); --xr-border-color: #1F1F1F; --xr-disabled-color: #515151; --xr-background-color: #111111; --xr-background-color-row-even: #111111; --xr-background-color-row-odd: #313131; } .xr-wrap { display: block !important; min-width: 300px; max-width: 700px; } .xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } <xarray.Dataset> Dimensions: (chain: 2, draw: 1000, like_dim_0: 1) Coordinates: * chain (chain) int64 0 1 * draw (draw) int64 0 1 2 3 4 5 6 7 ... 992 993 994 995 996 997 998 999 * like_dim_0 (like_dim_0) int64 0 Data variables: like (chain, draw, like_dim_0) float64 1.587e+03 ... 1.584e+03 Attributes: created_at: 2022-07-23T14:10:04.397013 arviz_version: 0.12.1 inference_library: pymc inference_library_version: 4.0.1 xarray.Dataset Dimensions: chain : 2 draw : 1000 like_dim_0 : 1 Coordinates: (3) chain (chain) int64 0 1 array([0, 1]) draw (draw) int64 0 1 2 3 4 5 ... 995 996 997 998 999 array([ 0, 1, 2, ..., 997, 998, 999]) like_dim_0 (like_dim_0) int64 0 array([0]) Data variables: (1) like (chain, draw, like_dim_0) float64 1.587e+03 1.579e+03 ... 1.584e+03 array([[[1587.07786376], [1579.22142173], [1591.73356013], ..., [1601.59229977], [1578.46236853], [1591.30863738]], [[1590.83753783], [1573.04613255], [1574.44687758], ..., [1575.81574783], [1576.57422809], [1584.35188303]]]) Attributes: (4) created_at : 2022-07-23T14:10:04.397013 arviz_version : 0.12.1 inference_library : pymc inference_library_version : 4.0.1 fig , axes = plt . subplots ( figsize = ( 10 , 5 ), ncols = 2 ) az . plot_posterior ( ppc_or . posterior_predictive [ \"like\" ], ax = axes [ 0 ]) axes [ 0 ] . set_title ( \"Original\" ) az . plot_posterior ( ppc_cf . posterior_predictive [ \"like\" ], ax = axes [ 1 ]) axes [ 1 ] . set_title ( \"Counterfactual\" ) Text(0.5, 1.0, 'Counterfactual') What's really cool here is that we've done a personalized counterfactual for that particular student. In this case, we set the cost of the student's tuition fees to 1/10 the original, akin to sending them from a private school to a public school. Under this particularly flawed model, we would expect their sum of grades to go down, though not by a large magnitude. The key trick to enabling this was using pm.Data() containers for our data, thus registering them with PyMC (and Aesara underneath) as being hot-swappable entities with pm.set_data() . Finally, because our posterior distributions contain our probabilistic beliefs having seen the data, or more colloquially, \"fits of our parameters\", we sample from the posterior predictive distribution to identify what we would have gotten in a counterfactual situation.","title":"Personalized counterfactuals in PyMC"},{"location":"07-do-operator/#the-do-operator-global-counterfactuals","text":"Let\u2019s also think about another possible counterfactual question: What if the system parameters were different? Recall here that \u201csystem parameters\u201d refer to the linear coefficients. They aren\u2019t properties of any observation (or sample), but are properties of the entire system as a whole, hence the moniker \u201cglobal\u201d counterfactuals. To do global counterfactuals, we actually need to condition the coefficients\u2019 values on a particular value, just like we did for data on the personalized counterfactual. For example, if we conditioned \\beta \\beta to be equal to 0, in other words severing the relationship between a a and b b , then: We are now asking for P(a|do(\\beta=0), \\gamma, b, c) P(a|do(\\beta=0), \\gamma, b, c) . (Notice the omission of subscript i i , it\u2019s intentional!) Our counterfactual question presumes a known value of \\beta \\beta , but not a presumed known value of any of (b, c) (b, c) . (We aren't asking about any particular sample, after all!) One way we can implement this do-operation is to reconstruct the model from our posteriors while hard-coding the value of \\beta \\beta to 0. Let's see how to make this happen. Firstly, in order to take advantage of our fitted posteriors, we will use a from_posterior function written implemented in the PyMC how-to guides. from scipy import stats # Credit: https://docs.pymc.io/projects/examples/en/latest/pymc3_howto/updating_priors.html def from_posterior ( param , samples ): smin , smax = np . min ( samples ), np . max ( samples ) width = smax - smin x = np . linspace ( smin , smax , 100 ) y = stats . gaussian_kde ( samples )( x ) # what was never sampled should have a small probability but not 0, # so we'll extend the domain and use linear approximation of density on it x = np . concatenate ([[ x [ 0 ] - 1 * width ], x , [ x [ - 1 ] + 1 * width ]]) y = np . concatenate ([[ 0 ], y , [ 0 ]]) return pm . Interpolated ( param , x , y ) Then, we'll need to rewrite the model a little bit, this time hard-coding one of the variables to a particular value. with pm . Model () as model_a_modified : a = pm . Data ( \"a\" , data [ \"a\" ]) b = pm . Data ( \"b\" , data [ \"b\" ]) c = pm . Data ( \"c\" , data [ \"c\" ]) # Model parameters beta = 0 # ****This is the do-operator in action!**** gamma = from_posterior ( \"gamma\" , idata_a . posterior [ \"gamma\" ] . values . flatten ()) sigma_a = from_posterior ( \"sigma_a\" , idata_a . posterior [ \"sigma_a\" ] . values . flatten ()) pm . Normal ( \"like\" , mu = beta * b + gamma * c , sigma = sigma_a , observed = a ) /usr/share/miniconda3/envs/causality/lib/python3.9/site-packages/pymc/data.py:667: FutureWarning: The `mutable` kwarg was not specified. Currently it defaults to `pm.Data(mutable=True)`, which is equivalent to using `pm.MutableData()`. In v4.1.0 the default will change to `pm.Data(mutable=False)`, equivalent to `pm.ConstantData`. Set `pm.Data(..., mutable=False/True)`, or use `pm.ConstantData`/`pm.MutableData`. warnings.warn( Finally, since our posteriors have become priors in this new model, we sample from the prior predictive distribution: with model_a_modified : trace = pm . sample_prior_predictive () /usr/share/miniconda3/envs/causality/lib/python3.9/site-packages/pymc/distributions/continuous.py:3704: RuntimeWarning: divide by zero encountered in double_scalars np.where(np.abs(pdf[index]) <= 1e-8, np.zeros(index.shape), (p - cdf[index]) / pdf[index]), Now, we're able to examine how our observations have changed based on the intervention at the system. def plot_global_counterfactual ( idx : int ): az . plot_posterior ( trace . prior_predictive [ \"like\" ][ 0 , :, idx ]) plt . gca () . set_title ( f \"Original value: { data . loc [ idx , 'a' ] : .2f } \" ) plot_global_counterfactual ( 2 ) plot_global_counterfactual ( 3 ) We can see here that by severing the connection between the IQ of a student and their grades, each student's total sum of grades goes down to 1/20th of their original. Those of us smarter than this author would probably have intuited this point without needing to code it up (by examining the magnitude of the slope coefficients), but in the case of exotic functional forms with interaction terms (or a neural net structure), the value of an explicit, global perturbation of an interpretable parameter is quite evident!","title":"The do-operator: global counterfactuals"},{"location":"07-do-operator/#the-do-operator-combining-global-and-personalized-counterfactuals","text":"If we\u2019re thinking logically here, we\u2019ll soon realize that it\u2019s also possible to combine the two aforementioned counterfactuals together. We can ask the question, what is P(a_i | do(\\beta=0, b_i=B), \\gamma, c_i) P(a_i | do(\\beta=0, b_i=B), \\gamma, c_i) ? (Note again the presence of the index i i !) Algorithmically, this question essentially translates to: picking out sample i i , setting b=B b=B , setting beta=3.14 beta=3.14 , and evaluating what a_i a_i would look like under those two conditions. For the sake of illustration, here it is in PyMC code: with model_a_modified : pm . set_data ({ \"a\" : sample [ \"a\" ], \"b\" : sample [ \"b\" ], \"c\" : sample [ \"c\" ]}) ppc_or = pm . sample_prior_predictive () # cf = \"counterfactual\" pm . set_data ({ \"a\" : sample [ \"a\" ], \"b\" : sample [ \"b\" ], \"c\" : sample [ \"c\" ] * 0.1 }) ppc_cf = pm . sample_prior_predictive () fig , axes = plt . subplots ( figsize = ( 15 , 5 ), ncols = 2 ) az . plot_posterior ( ppc_or . prior_predictive [ \"like\" ], ax = axes [ 0 ]) axes [ 0 ] . set_title ( r \"Distribution of $a$ with $\\beta=0$ and $c = c_i$\" ) az . plot_posterior ( ppc_cf . prior_predictive [ \"like\" ], ax = axes [ 1 ]) axes [ 1 ] . set_title ( r \"Distribution of $a$ with $\\beta=0$ and $c = \\frac {c_i}{10} $\" ) Text(0.5, 1.0, 'Distribution of $a$ with $\\\\beta=0$ and $c = \\\\frac{c_i}{10}$')","title":"The do-operator: combining global and personalized counterfactuals"},{"location":"07-do-operator/#framework","text":"Having worked through this example, we've finally arrived at a framework for thinking through the connection between Bayesian models and causal models. To know which kind of counterfactual we need to employ, we have to be extremely clear on the exact question we\u2019re trying to ask. Are we trying to ask: A personalized question? (\u201dWhat would have happened to this particular sample had its dependent variable been set to a particular value?\u201d) A systems-level question? (\u201dWhat would happen to all of our observations if a system parameter was set to a particular value?\u201d) A hybrid question? (\u201dWhat would have happened to this particular sample had its dependent variable been set to a particular value and the system parameter set to a particular value?\u201d) If we have our variables\u2019 dependencies clearly and explicitly stated, then it becomes easy to ask the do-operator family of questions, which basically are asking, \u201cWhat happens if we set something in the model to a particular value?\u201d","title":"Framework"},{"location":"07-do-operator/#summary","text":"In this notebook, we have seen how to do causal inference on Bayesian models written in PyMC. We started with the contrast and comparison between causal and Bayesian inference. Then, we went through the logical framework of asking counterfactuals on (1) a per-sample basis, (2) a system-wide basis, and (3) both together. The value of the do-operator, and by extension causal methods, is that they provide us the ability to ask these \"what if\" questions in cases where conducting an experiment would be unethical, cost-prohibitive, or logistically challenging. Combining it with Bayesian models gives us the ability to assess what would have happened in a counterfactual world (causal) while also calculating the full range of possibilities (Bayesian). Blending the two together is nothing more than a matter of logic!","title":"Summary"},{"location":"07-do-operator/#credits","text":"I would like to credit Thomas Wiecki and Ben Vincent for reviewing an early draft of this notebook.","title":"Credits"}]}