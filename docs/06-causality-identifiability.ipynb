{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directional Identifiability\n",
    "\n",
    "From Jonas Peters' lecture  3  on causality at the Broad.\n",
    "\n",
    "If we have a presumed causal model of the linear form:\n",
    "\n",
    "$$Y = \\alpha X + N_y$$\n",
    "\n",
    "where $N_y$ is i.i.d. noise in $Y$, and $X$ and $N_y$ are both independent and non-Gaussian, then we cannot find\n",
    "\n",
    "$$X = \\beta Y + N_x$$\n",
    "\n",
    "where $N_x$ is i.i.d. noise in $X$ that also satisfies the independence constraints. \n",
    "\n",
    "In simpler words, if we assume that the distributions of $X$ and $N_y$ are non-Gaussian, then we will know that the causal model goes from $X \\rightarrow Y$ and not $Y \\rightarrow X$.\n",
    "\n",
    "Let's simulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will generate non-Gaussian Xs and Ys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ng = np.random.uniform(-1, 1, size=1000)\n",
    "alpha = 2\n",
    "N_y_ng = np.random.uniform(-0.4, 0.4, size=1000)\n",
    "y_ng = alpha * X_ng + N_y_ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot Y against X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_ng, y_ng)\n",
    "plt.ylabel(\"Y\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's also simulate the case where $X$ and $N_y$ are Gaussian-distributed and independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_g = np.random.normal(0, 0.5, size=1000)\n",
    "alpha_g = 2\n",
    "N_y_g = np.random.normal(0, 1, size=1000)\n",
    "y_g = alpha_g * X_g + N_y_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_g, y_g)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit X as a function of Y, and do a residual analysis to see whether our residuals (i.e. noise) are independent of the input (in this case Y). Remember, we are looking to check that the inverse condition holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we X as function of Y and obtain a coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(y_g.reshape(-1, 1), X_g)\n",
    "coeff_g = lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do the same for the non-Gaussian case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(y_ng.reshape(-1, 1), X_ng)\n",
    "coeff_ng = lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have the coefficients out, let's move on to the analysis of residuals. We will be checking that the residuals ($X - \\beta Y$) should be independent of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, the Gaussian case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_g = X_g - coeff_g * y_g\n",
    "plt.scatter(y_g, residuals_g)\n",
    "plt.xlabel(\"Y\")\n",
    "plt.ylabel(\"residual\")\n",
    "plt.title(\"Gaussian\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is no trend in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the non-gaussian case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_ng = X_ng - coeff_ng  * y_ng\n",
    "plt.scatter(y_ng, residuals_ng)\n",
    "plt.xlabel(\"Y\")\n",
    "plt.ylabel(\"residuals\")\n",
    "plt.title(\"non-Gaussian\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a clear trend  - residual depends on the value of y in the non-Gaussian case, whereas it does not in the Gaussian case.\n",
    "\n",
    "This empirical simulation illustrates how we cannot recover an inverse model where the noise in X ($N_x$) is independent of the value of $Y$. Hence, we have an **identifiable** model under non-Gaussian assumptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
